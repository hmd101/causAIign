#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CBN cross-experiment aggregator
================================

Purpose
-------
This script sweeps model-fit summaries produced by the causal Bayesian network (CBN)
pipeline and aggregates them across experiments and prompt-categories. It then
computes matched within-agent comparisons and (optionally) mixed-effects models,
and exports compact CSV and LaTeX artifacts, plus a few summary plots.

Input data layout
-----------------
The script expects per-experiment CBN analysis outputs under:

    results/parameter_analysis/<experiment>/<tag>/normat_analysis/

where:
- <experiment> is one of the experiments (e.g., "rw17_indep_causes", "random_abstract",
  "rw17_overloaded_de", "abstract_overloaded_lorem_de", ...), and
- <tag> encodes the CBN configuration and prompt-category, e.g.,
  "v1_noisy_or_pcnum_p3-4_lr0.1_noh" or "v1_noisy_or_pccot_p3-4_lr0.1_noh".

Within each normat_analysis/ folder, the primary file consumed is
"analysis_parameters_wide.csv" (preferred). If missing, the script falls back to
"winners_with_params.csv" (and optionally merges params_tying from "winners.csv").
These files are generated by upstream analysis scripts and contain per-agent metrics,
including at least the following numeric columns (if available):

- loocv_r2: Leave-one-out cross-validated R^2 of the CBN fit (higher is more normative)
- b, m1, m2: Abstract CBN parameters (normativity-relevant)
- pC1, pC2: Inferred p(C) parameters
- params_tying: The best-fitting model’s parameter-tying count (3 or 4)
- agent: model/agent identifier (string)
- domain (optional): either a domain label or "all" (pooled)
- family (optional): agent family label (e.g., model series)

What the script produces
------------------------
Given the discovered inputs, the script builds a "master table" (CSV) with one row per
(experiment, tag, prompt_category, agent), preferring pooled (domain == "all") rows
when both pooled and per-domain rows exist.

From this master table, the script produces:
1) Per-condition medians (CSV + optional LaTeX): per (experiment × prompt_category),
    medians of R^2 and CBN parameters, share of 3-parameter winners, and average tokens.
    This tells us which condition is more normative (higher R^2), which parameters are
    typically larger/smaller, and the prevalence of simpler (3-parameter) models.

2) Prompt-category within-experiment comparisons (CSV + optional plots/LaTeX):
    pcnum vs pccot matched by agent, with paired deltas per metric and Wilcoxon tests
    (BH–FDR adjusted). This reveals whether prompting with CoT vs Numeric changes
    normative fits (R^2) or parameters for the same agents.

3) Experiment vs baseline comparisons within each prompt-category (CSV + optional
    plots/LaTeX): for a selected baseline experiment (default: rw17_indep_causes),
    compare every other experiment, matched by agent, within a fixed prompt-category.
    This indicates whether, for the same agents and prompt-category, experimental
    manipulations (e.g., overloaded prompts) improve/worsen fit quality or shift
    parameter estimates.

4) Mixed-effects models (CSV + optional LaTeX):
    - R^2 MixedLM (or OLS-FE fallback) with agent random-effects/FE + cluster-robust SE.
     Interpreting coefficients tells us how experiments and prompt-categories relate to
     normative fits after pooling across unbalanced agent sets.
    - GLM Binomial for 3-parameter winner share, with agent FE and cluster-robust SE,
     indicating which conditions are associated with simpler models.

5) Summary plots (optional): grouped boxplots of R^2 by experiment × prompt_category
    and per-agent slope plots for paired comparisons.

CLI usage
---------
Run the script from the repository root (or any path) and point --root to
"results/parameter_analysis" if needed. By default, outputs are written under
"<root>/cbn_agg" with subfolders for each comparison type.

Acronym explanations
--------------------
- OLS-FE: Ordinary Least Squares with Fixed Effects. This is a linear regression model that includes fixed effects (dummy variables) for each agent to control for agent-specific differences.
- Random-effects/FE: "Random-effects" refers to mixed-effects models where agent effects are modeled as random variables. "FE" stands for "Fixed Effects", where agent effects are modeled as fixed dummy variables.
- Cluster-robust SE: Cluster-robust Standard Errors. These are standard errors that account for correlation within clusters (here, agents), making inference more reliable when data points within a cluster are not independent.

Author: causAIign
"""

from __future__ import annotations

import argparse
import hashlib
import re
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd
from matplotlib.lines import Line2D

# Optional statsmodels (mixed effects). If missing, we fallback.
sm: Any = None  # for type checkers if statsmodels is missing
smf: Any = None
try:
    import statsmodels.api as sm  # type: ignore[assignment]
    import statsmodels.formula.api as smf  # type: ignore[assignment]
    HAS_SM = True
except Exception:
    HAS_SM = False

# Matplotlib for plots (no seaborn, single-plot PDFs).
import matplotlib  # noqa: E402

matplotlib.use("Agg")
import matplotlib as mpl  # noqa: E402
import matplotlib.pyplot as plt  # noqa: E402
from matplotlib.patches import Patch  # noqa: E402
from tueplots import bundles, fonts  # noqa: E402

# from tueplots.figsizes import rel_width as tw_rel_width

# w, _ = tw_rel_width(rel_width=0.9)  # 90% of a NeurIPS column width
# fig, ax = plt.subplots(figsize=(w, height))


# NeurIPS-like, LaTeX, serif
# config = bundles.neurips2023(nrows=2, ncols=1, rel_width=0.3, usetex=True, family="serif")

config = bundles.neurips2023(
    nrows=2, ncols=1,
    rel_width=0.8,   # <- was 0.3
    usetex=True, family="serif"
)


config["legend.title_fontsize"] = 9
config["font.size"] = 14
config["axes.labelsize"] = 14
config["axes.titlesize"] = 12
config["xtick.labelsize"] = 12
config["ytick.labelsize"] = 12
config["legend.fontsize"] = 9
config["text.latex.preamble"] = r"\usepackage{amsmath,bm,xcolor} \definecolor{inference}{HTML}{FF5B59}"

font_config = fonts.neurips2022_tex(family="serif")
config = {**config, **font_config}

mpl.rcParams.update(config)


# ---  helpers for highlighting and legends --------------------------------
# Define experiment name mapping
# exp_name_map = {
#     "random_abstract": "Abstract",
#     "rw17_indep_causes": "RW17",
#    "abstract_overloaded_lorem_de": "Abstract-Over-DE",
#    "rw17_overloaded_de": "RW17-Over-DE",
#    "rw17_overloaded_d": "RW17-Over-D",
#    "rw17_overloaded_e": "RW17-Over-E"
# }

# Mapping when only one overloaded experiment for abstract / overloaded 
exp_name_map = {
    "random_abstract": "Abstract",
    "rw17_indep_causes": "RW17",
   "abstract_overloaded_lorem_de": "Abstract-Over",
   "rw17_overloaded_de": "RW17-Over-DE",
   "rw17_overloaded_d": "RW17-Over-D",
   "rw17_overloaded_e": "RW17-Over"
}

# Pretty label mapping for prompt categories (publication labels)
PC_PRINT_MAP = {"pcnum": "Num", "pccot": "CoT", "pcconf": "Num-Conf"}

try:
    from causalign.plotting.palette import PROMPT_CATEGORY_COLORS, canon_prompt_category
except Exception:
    # Fallback if src/ not on path when running the script directly
    PROMPT_CATEGORY_COLORS = {
        "numeric": (0.85, 0.60, 0.55),
        "CoT": (0.00, 0.20, 0.55),
    }
    def canon_prompt_category(label: str) -> str:  # type: ignore
        t = str(label).strip().lower()
        if t in NUMERIC_SYNS or t == "numeric":
            return "numeric"
        if t in COT_SYNS or t == "cot":
            return "CoT"
        return str(label)

# Assign colors from PROMPT_CATEGORY_COLORS
numeric_color = PROMPT_CATEGORY_COLORS["numeric"]
cot_color = PROMPT_CATEGORY_COLORS["CoT"]

from tueplots import bundles, fonts


# Global, deterministic agent color mapping across figures (stable across runs)
def _build_palette() -> list:
    import matplotlib.cm as cm

    # Combine multiple tab palettes and HSV sampling for many distinct colors
    palettes = []
    for name in ["tab20", "tab20b", "tab20c"]:
        try:
            cmap = matplotlib.colormaps.get_cmap(name)
        except Exception:
            # Fallback for older Matplotlib versions
            cmap = plt.get_cmap(name)
        palettes.extend([cmap(i) for i in range(cmap.N)])
    # Add extra HSV samples to increase unique colors
    # Additional hues via HSV sampling using Matplotlib's colors conversion
    import matplotlib.colors as mcolors
    for i in range(40):
        rgb = mcolors.hsv_to_rgb((i / 40.0, 0.85, 0.9))
        palettes.append((*rgb, 1.0))
    # Convert RGBA tuples to RGB (drop alpha)
    rgb = [(r, g, b) for (r, g, b, _a) in palettes]
    return rgb

_AGENT_COLOR_PALETTE = _build_palette()

def agent_color(a: str) -> tuple:
    """Stable color for an agent name using MD5-based index into a large palette."""
    h = int(hashlib.md5(str(a).encode("utf-8")).hexdigest(), 16)
    return _AGENT_COLOR_PALETTE[h % len(_AGENT_COLOR_PALETTE)]

def agent_style(a: str) -> Dict[str, Any]:
    """Stable per-agent style: color + linestyle + marker for better distinguishability."""
    h = int(hashlib.md5(str(a).encode("utf-8")).hexdigest(), 16)
    color = _AGENT_COLOR_PALETTE[h % len(_AGENT_COLOR_PALETTE)]
    line_styles = ["-", "--", ":", "-."]
    markers = ["o", "s", "^", "D", "v", "P", "X"]
    ls = line_styles[h % len(line_styles)]
    mk = markers[(h // len(line_styles)) % len(markers)]
    return {"color": color, "linestyle": ls, "marker": mk}





###### token average for prompt categories(experiment x prompt-category)

"""
This is how tokens are computed in src.causalign.prompts.core.token_utils.py:

    Count tokens in a prompt or list of prompts using whitespace split.
    Returns the token count (int) or list of counts (List[int]).

    if isinstance(prompt, str):
        return len(prompt.split())
    elif isinstance(prompt, list):
        return [len(p.split()) for p in prompt]
    else:
        raise TypeError("Input must be a string or list of strings.")
"""

token_avg = {
    "random_abstract": {
        "numeric-conf": 200.8,
        "numeric": 134.8,
        "CoT": 234.8,
    },
    "abstract_overloaded_lorem_de": {
        "numeric-conf": 500.8,
        "numeric": 434.8,
        "CoT": 534.8,
    },
    "rw17_indep_causes": {
        "numeric": 268.6333333333333,
        "numeric-conf": 334.6333333333333,
        "CoT": 368.6333333333333,
    },
    "rw17_overloaded_lorem_de": {
        # no data in text block
    },
    "rw17_overloaded_de": {
        "numeric-conf": 434.09166666666664,
        "numeric": 368.09166666666664,
        "CoT": 468.09166666666664,
    },
    "rw17_overloaded_d": {
        "numeric-conf": 383.46666666666664,
        "numeric": 317.46666666666664,
        "CoT": 417.46666666666664,
    },
    "rw17_overloaded_e": {
        "numeric-conf": 377.59166666666664,
        "numeric": 311.59166666666664,
        "CoT": 411.59166666666664,
    }
}



######### prompt category custom colors
# numeric_color = (0.85, 0.60, 0.55)  # #D9998C
# cot_color     = (0.00, 0.20, 0.55)  # #00338C

def _qtiles(series, qs=(0.25, 0.75)):
    s = pd.to_numeric(series, errors="coerce").dropna()
    if s.empty:
        return (float("nan"), float("nan"))
    return (float(np.nanquantile(s, qs[0])), float(np.nanquantile(s, qs[1])))

def _top_bottom_agents_for_baseline(df_baseline: pd.DataFrame, max_legend_agents: int = 8):
    """
    Given a dataframe with columns ['agent','loocv_r2'], compute agents in
    bottom quartile (<= Q1) and top quartile (>= Q3) of LOOCV R².

    Returns:
      bottom_set, top_set, legend_bottom, legend_top
    where legend_* are (possibly) truncated lists capped at max_legend_agents/2 each
    to keep the legend compact.
    """
    r2 = pd.to_numeric(df_baseline["loocv_r2"], errors="coerce")
    q1, q3 = _qtiles(r2)
    keep = df_baseline.assign(_r2=r2).dropna(subset=["_r2"])
    bottom = keep[keep["_r2"] <= q1].sort_values("_r2").agent.tolist()
    top    = keep[keep["_r2"] >= q3].sort_values("_r2", ascending=False).agent.tolist()

    # Cap legend size to avoid clutter
    max_each = max_legend_agents // 2
    legend_bottom = bottom[:max_each]
    legend_top    = top[:max_each]
    return set(bottom), set(top), legend_bottom, legend_top

def _agent_color_map(agents: list[str]):
    """
    Deterministic color assignment for highlighted agents using tab10 palette.
    """
    import itertools as _it
    cmap = plt.get_cmap("tab10")
    # some Colormap types expose .colors, otherwise sample
    base = list(getattr(cmap, "colors", [cmap(i) for i in range(getattr(cmap, "N", 10))]))
    cyc = _it.cycle(base)
    return {a: next(cyc) for a in agents}

def _boxplot_legend_handles():
    """Proxy artists describing the box, whiskers, circles, triangles."""
    return [
        # Patch(facecolor="#D9998C", edgecolor="black", label="Distribution (box): pcnum"),
        # Patch(facecolor="#00338C", edgecolor="black", label="Distribution (box): pccot"),
        Patch(facecolor=PROMPT_CATEGORY_COLORS["numeric"], edgecolor="black", label="Distribution (box): pcnum"),
        Patch(facecolor=PROMPT_CATEGORY_COLORS["CoT"], edgecolor="black", label="Distribution (box): pccot"),
    # prompt category colors used in human-llm alignment plots:
    #  numeric_color = (0.85, 0.60, 0.55)
    # cot_color     = (0.00, 0.20, 0.55)
        Line2D([0], [0], color="black", lw=1.2, label="Black line: median"),
        Line2D([0], [0], color="black", lw=1.0, linestyle="-", label="Whiskers: 1.5xIQR"),
        Line2D([0], [0], marker="o", linestyle="none", markersize=6, markerfacecolor="gray",
               alpha=0.6, label="Agent $R^2$ (circles)"),
        Line2D([0], [0], marker="^", linestyle="none", markersize=8, markerfacecolor="yellow",
               label="Condition median (triangles)"),
    ]


# ------------------------- Discovery & I/O -------------------------

PROJECT_ROOT = Path(__file__).resolve().parents[0]

def find_normat_dirs(root: Path, experiments: Optional[List[str]] = None) -> List[Path]:
    """Discover all normat_analysis directories.

    Parameters
    ----------
    root : Path
        Root directory that contains per-experiment folders (e.g., results/parameter_analysis).
    experiments : list[str] | None
        Optional whitelist of experiment folder names to include.

    Returns
    -------
    list[pathlib.Path]
        Paths to "normat_analysis" folders under: root/<experiment>/<tag>/normat_analysis.

    Notes
    -----
    This locates the input data for aggregation. Each normat_analysis directory
    is expected to contain analysis files such as analysis_parameters_wide.csv.
    """
    found: List[Path] = []
    if not root.exists():
        return found
    for exp_dir in sorted([p for p in root.iterdir() if p.is_dir()]):
        if experiments and exp_dir.name not in experiments:
            continue
        for tag_dir in sorted([p for p in exp_dir.iterdir() if p.is_dir()]):
            na = tag_dir / "normat_analysis"
            if na.exists() and na.is_dir():
                found.append(na)
    return found


_PC_RE = re.compile(r'(?:(?:^|[\\/_-])(pcnum|pccot)(?:[\\/_-]|$))', re.IGNORECASE)

def detect_prompt_category_from_tag(tag_path: Path) -> str:
    """Infer prompt-category from a tag directory name.

    Accepts both underscores and hyphens.

    Examples
    --------
    - v1_noisy_or_pccot_p3-4_lr0.1_noh -> "pccot"
    - v1-noisy-or-pcnum-p3-4-lr0.1-noh -> "pcnum"

    Returns
    -------
    str
        One of {"pcnum", "pccot", "unknown"}.
    """
    name = tag_path.name
    m = _PC_RE.search(name)
    return m.group(1).lower() if m else "unknown"


def read_parameters_wide(normat_dir: Path) -> Optional[pd.DataFrame]:
    """Load per-agent metrics for a single (experiment, tag) condition.

    Priority is given to analysis_parameters_wide.csv in the provided
    normat_analysis directory. If absent, fall back to winners_with_params.csv in
    the tag directory and merge params_tying from winners.csv when needed.

    Expected columns (if present):
    - agent (str), domain (str or "all"), loocv_r2, b, m1, m2, pC1, pC2, params_tying
    - optional: family

    Returns
    -------
    pandas.DataFrame | None
        The loaded dataframe (possibly subset of the expected columns) or None
        if no suitable files are found.
    """
    pref = normat_dir / "analysis_parameters_wide.csv"
    if pref.exists():
        df = pd.read_csv(pref)
        return df

    # Fallback to winners in tag parent
    tag_dir = normat_dir.parent
    wwp = tag_dir / "winners_with_params.csv"
    win = tag_dir / "winners.csv"
    if wwp.exists():
        df = pd.read_csv(wwp)
        df["domain"] = df["domain"].where(df["domain"].notna(), "all")
        if ("params_tying" not in df.columns) and win.exists():
            w = pd.read_csv(win)
            w["domain"] = w["domain"].where(w["domain"].notna(), "all")
            meta = w[["agent", "domain", "params_tying"]].drop_duplicates(["agent", "domain"])
            df = df.merge(meta, on=["agent", "domain"], how="left")
        return df
    return None



def parse_prompt_category_from_tag(tag: str) -> Optional[str]:
    """Robustly infer prompt category from a tag name.

    Recognizes common spellings and separators.

    Returns
    -------
    str | None
        One of {"pcnum", "pccot", "pcconf"} or None if no match.
    """
    s = str(tag).lower()
    # Normalize separators
    s_norm = s.replace("-", "_")
    tokens = set(s_norm.split("_"))
    # Direct checks
    if "pcnum" in s_norm or ("num" in tokens and "pc" in tokens) or "numeric" in tokens:
        return "pcnum"
    if "pccot" in s_norm or "cot" in tokens or "chainofthought" in s_norm or "chain_of_thought" in s_norm:
        return "pccot"
    if "pcconf" in s_norm or "pcnconf" in s_norm or "conf" in tokens:
        return "pcconf"
    return 


def infer_prompt_category(tag_dir: Path, df: Optional[pd.DataFrame]) -> str:
    """Infer prompt-category using multiple sources.

    Priority order: (1) a single value in df['prompt_category'], (2) tag_dir/manifest.json,
    (3) tag directory name heuristics. Falls back to "unknown".

    Returns
    -------
    str
        "pcnum", "pccot", "pcconf", or "unknown".
    """
    # 1) If DF carries a single prompt_category value, use it
    try:
        if df is not None and "prompt_category" in df.columns:
            vals = [str(v).strip().lower() for v in df["prompt_category"].dropna().unique().tolist()]
            if len(vals) == 1 and vals[0]:
                if vals[0] in {"pcnum","pccot","pcconf"}:
                    return vals[0]
                # map common aliases
                if vals[0] in {"numeric","num"}:
                    return "pcnum"
                if vals[0] in {"cot","chain-of-thought","chain_of_thought","chainofthought"}:
                    return "pccot"
    except Exception:
        pass
    # 2) manifest.json (written by exporters in some pipelines)
    try:
        manifest = tag_dir / "manifest.json"
        if manifest.exists():
            import json as _json
            m = _json.loads(manifest.read_text())
            pc = str(m.get("prompt_category", "")).strip().lower()
            if pc:
                if pc in {"pcnum","pccot","pcconf"}:
                    return pc
                if pc in {"numeric","num"}:
                    return "pcnum"
                if pc in {"cot","chain-of-thought","chain_of_thought","chainofthought"}:
                    return "pccot"
    except Exception:
        pass
    # 3) Parse from tag string
    pc = parse_prompt_category_from_tag(tag_dir.name)
    return pc if pc is not None else "unknown"


# ------------------------- Utilities -------------------------
# --- token helpers ---
_PC_TO_TOKENKEY = {"pcnum": "numeric", "pccot": "CoT", "pcconf": "numeric-conf"}

def avg_tokens_for(exp: str, pc: str) -> float:
    """Lookup floored average prompt tokens for an (experiment, prompt-category).

    Returns
    -------
    float
        Floored average token count, or NaN if unavailable.
    Notes
    -----
    A convenience to annotate plots/tables with approximate prompt length differences.
    """
    try:
        key = _PC_TO_TOKENKEY[str(pc).lower()]
        val = token_avg.get(exp, {}).get(key, np.nan)
        return np.floor(val) if pd.notna(val) else np.nan
    except KeyError:
        return np.nan


def _pretty_param_label(param: str) -> str:
    mplabels = {"b": "$b$", "m1": "$m_1$", "m2": "$m_2$", "pCavg": "$p(C)$"}
    return mplabels.get(param, param)




def latex_escape(s: str) -> str:
    """Escape a string for safe LaTeX table rendering.

    Replaces common special characters with LaTeX-safe equivalents.
    """
    if s is None:
        return ""
    out = str(s)
    repl = {
        "\\": r"\textbackslash{}",
        "&": r"\&",
        "%": r"\%",
        "_": r"\_",
        "#": r"\#",
        "{": r"\{",
        "}": r"\}",
    }
    for k, v in repl.items():
        out = out.replace(k, v)
    return out



def to_num(s):
    """Coerce a pandas Series/array-like to numeric, setting invalids to NaN."""
    return pd.to_numeric(s, errors="coerce")


def _iqr_summary(vals: Iterable[float]) -> Dict[str, float]:
    """Compute median, quartiles, IQR, whisker fences and min/max for an array-like.

    Returns a dict with keys: N, median, q1, q3, iqr, lower_fence, upper_fence, vmin, vmax.
    """
    x = pd.to_numeric(pd.Series(list(vals), dtype=float), errors="coerce").dropna()
    if x.empty:
        return {
            "N": 0, "median": float("nan"), "q1": float("nan"), "q3": float("nan"),
            "iqr": float("nan"), "lower_fence": float("nan"), "upper_fence": float("nan"),
            "vmin": float("nan"), "vmax": float("nan"),
        }
    q1 = float(np.nanquantile(x, 0.25))
    q3 = float(np.nanquantile(x, 0.75))
    iqr = float(q3 - q1)
    return {
        "N": int(x.size),
        "median": float(np.nanmedian(x)),
        "q1": q1,
        "q3": q3,
        "iqr": iqr,
        "lower_fence": float(q1 - 1.5 * iqr),
        "upper_fence": float(q3 + 1.5 * iqr),
        "vmin": float(np.nanmin(x)),
        "vmax": float(np.nanmax(x)),
    }


def bh_fdr(pvals: Iterable[float], alpha: float = 0.05) -> Tuple[np.ndarray, np.ndarray]:
    """Benjamini–Hochberg multiple testing correction.

    Parameters
    ----------
    pvals : Iterable[float]
        Raw p-values to adjust.
    alpha : float, default 0.05
        FDR target.

    Returns
    -------
    (np.ndarray, np.ndarray)
        Tuple (reject_flags, adjusted_p_values), each aligned to pvals.
    """
    p = np.asarray(list(pvals), dtype=float)
    n = len(p)
    if n == 0:
        return np.array([], dtype=bool), np.array([])
    order = np.argsort(p)
    ranked = p[order]
    adj = np.empty_like(ranked)
    denom = np.arange(1, n + 1)
    adj_vals = ranked * n / denom
    adj[::-1] = np.minimum.accumulate(adj_vals[::-1])
    adjusted = np.empty_like(adj)
    adjusted[order] = adj
    reject = adjusted <= alpha
    return reject, adjusted


def share_three_param(series: pd.Series) -> str:
    """Return simple "num/den" share of 3-parameter winners in a series.

    Interprets values equal to 3 as a 3-parameter winner.
    """
    s = to_num(series).dropna()
    if s.empty:
        return "--"
    num = int((s == 3).sum())
    den = int(len(s))
    return f"{num}/{den}"


def _bootstrap_ci(x: np.ndarray, q: float = 0.5, reps: int = 1000, alpha: float = 0.05, rng_seed: int = 1337) -> Tuple[float, float]:
    """Bootstrap percentile CI for a quantile (default median)."""
    x = x[np.isfinite(x)]
    if x.size == 0:
        return (np.nan, np.nan)
    rng = np.random.default_rng(rng_seed)
    qs = []
    n = x.size
    for _ in range(reps):
        samp = rng.choice(x, size=n, replace=True)
        qs.append(np.nanquantile(samp, q))
    lo = float(np.nanquantile(qs, alpha / 2))
    hi = float(np.nanquantile(qs, 1 - alpha / 2))
    return lo, hi
# ------------------------- Master table build -------------------------

REQUIRED = ["agent", "loocv_r2", "b", "m1", "m2", "pC1", "pC2", "params_tying"]

def build_master(root: Path, experiments: Optional[List[str]] = None) -> pd.DataFrame:
    """Assemble the cross-experiment master table from normat_analysis inputs.

    Parameters
    ----------
    root : Path
        Root containing <experiment>/<tag>/normat_analysis/ (e.g., results/parameter_analysis).
    experiments : list[str] | None
        Optional list to restrict which experiments are included.

    Returns
    -------
    pandas.DataFrame
        One row per (experiment, tag, prompt_category, agent). If domain-specific and pooled
        rows co-exist, the pooled (domain=="all") row is preferred. Columns include:
        agent, loocv_r2, b, m1, m2, pC1, pC2, params_tying, experiment, tag, prompt_category,
        optional family, and a convenience abs_mdiff = |m1 - m2|.

    What this reveals
    -----------------
    The master table is the foundation for all comparisons. Inspecting it directly
    (master_table.csv) allows quick checks of coverage: which agents, experiments,
    and prompt-categories are present, and whether numeric columns have plausible ranges.
    """
    rows: List[pd.DataFrame] = []
    for normat_dir in find_normat_dirs(root, experiments):
        tag_dir = normat_dir.parent
        exp = tag_dir.parent.name
        tag = tag_dir.name
    # Determine prompt-category via tag directory name

        df = read_parameters_wide(normat_dir)
        if df is None or df.empty:
            continue

        # Coerce required columns
        for c in REQUIRED:
            if c not in df.columns:
                df[c] = np.nan
            df[c] = to_num(df[c]) if c != "agent" else df[c]

        # Standardize fields we care about
        keep = ["agent", "domain", "loocv_r2", "b", "m1", "m2", "pC1", "pC2", "params_tying"]
        # include family if present for later subgroup views
        if "family" in df.columns:
            keep.append("family")
        sub = df[keep].copy()
        sub["experiment"] = exp
        sub["tag"] = tag
        pc_final = detect_prompt_category_from_tag(tag_dir)
        # pc_final = infer_prompt_category(tag_dir, df)
        sub["prompt_category"] = pc_final if pc_final else "unknown"
        sub["abs_mdiff"] = (sub["m1"] - sub["m2"]).abs()
        rows.append(sub)

    master = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=REQUIRED + ["experiment", "tag", "prompt_category"])
    # one row per (experiment, prompt_category, tag, agent); if there are multiple domains, prefer pooled/all if present
    if "domain" in master.columns:
        # Prefer pooled/all over domain-specific for the master; if multiple, keep the row with domain == 'all'
        master["domain_norm"] = master["domain"].astype(str).str.lower()
        master = master.sort_values(by=["domain_norm"], key=lambda s: (s != "all"))  # 'all' first
        master = master.drop_duplicates(["experiment", "prompt_category", "tag", "agent"], keep="first")
        master = master.drop(columns=["domain_norm"], errors="ignore")
    return master


# ------------------------- Paired comparisons -------------------------
def make_prompt_category_pairs_for_all_experiments(master: pd.DataFrame, out_root: Path, do_plots: bool = True, export_tex: bool = False) -> None:
    """Within-experiment pcnum vs pccot comparisons (matched by agent).

    For each experiment that has both prompt-categories, compute paired deltas for
    key metrics and save artifacts under:
        <out_root>/prompt_category_pairs/<experiment>/

    Outputs (CSV)
    -------------
    - pcnum_rows.csv, pccot_rows.csv: aligned rows for agents in the intersection.
    - paired_deltas_pcnum_to_pccot.csv: per-agent deltas for metrics (pccot - pcnum).
    - paired_summary_pcnum_to_pccot.csv: small summary of delta distributions.
    - summary_pcnum_vs_pccot.csv: medians and N per condition.

    Optional
    --------
    - PDF: box_R2_pcnum_vs_pccot.pdf (R^2 distributions by prompt-category)
    - LaTeX: summary_pcnum_vs_pccot.tex

    What this reveals
    -----------------
    Whether prompting with CoT (pccot) changes fit quality or inferred parameters
    relative to numeric prompting (pcnum) for the very same agents, controlling
    for agent-level differences. The CSVs provide both the raw paired deltas and
    simple summaries that can be inspected or reanalyzed downstream.
    """
    base = out_root / "prompt_category_pairs"
    base.mkdir(parents=True, exist_ok=True)

    for exp, sub in master.groupby("experiment"):
        pcs = set(sub["prompt_category"].dropna().astype(str).str.lower())
        if not {"pcnum", "pccot"}.issubset(pcs):
            print(f"[INFO] Skipping pcnum↔pccot for {exp}: missing one prompt-category.")
            continue

        sub_pcnum = sub[sub["prompt_category"].str.lower() == "pcnum"].copy()
        sub_pccot = sub[sub["prompt_category"].str.lower() == "pccot"].copy()

        # intersect by agent for paired tests
        common_agents = sorted(set(sub_pcnum["agent"]) & set(sub_pccot["agent"]))
        if not common_agents:
            print(f"[INFO] {exp}: pcnum↔pccot has no matched agents; writing unpaired summaries only.")

        out_dir = base / str(exp)
        out_dir.mkdir(parents=True, exist_ok=True)

        # Save raw aligned tables
        pcnum_aligned = sub_pcnum[sub_pcnum["agent"].isin(common_agents)].copy()
        pccot_aligned = sub_pccot[sub_pccot["agent"].isin(common_agents)].copy()

        pcnum_aligned.to_csv(out_dir / "pcnum_rows.csv", index=False)
        pccot_aligned.to_csv(out_dir / "pccot_rows.csv", index=False)

        # Summaries by condition
        def _med(df, col): return float(pd.to_numeric(df[col], errors="coerce").median(skipna=True))
        med_pcnum = _med(sub_pcnum, "loocv_r2") if "loocv_r2" in sub_pcnum.columns else float("nan")
        med_pccot = _med(sub_pccot, "loocv_r2") if "loocv_r2" in sub_pccot.columns else float("nan")
        pd.DataFrame({
            "experiment": [exp, exp],
            "prompt_category": ["pcnum", "pccot"],
            "n_agents": [sub_pcnum["agent"].nunique(), sub_pccot["agent"].nunique()],
            "median_loocv_r2": [med_pcnum, med_pccot],
        }).to_csv(out_dir / "summary_pcnum_vs_pccot.csv", index=False)

        # Paired delta (if any)
        if common_agents:
            merged = (pcnum_aligned[["agent", "loocv_r2"]]
                        .merge(pccot_aligned[["agent", "loocv_r2"]], on="agent",
                               suffixes=("_pcnum","_pccot")))
            merged["delta_pccot_minus_pcnum"] = merged["loocv_r2_pccot"] - merged["loocv_r2_pcnum"]
            merged.to_csv(out_dir / "paired_deltas_pcnum_to_pccot.csv", index=False)

            # Small paired summary
            stats = {
                "n_pairs": len(merged),
                "median_delta": float(merged["delta_pccot_minus_pcnum"].median()),
                "mean_delta": float(merged["delta_pccot_minus_pcnum"].mean()),
                "min_delta": float(merged["delta_pccot_minus_pcnum"].min()),
                "max_delta": float(merged["delta_pccot_minus_pcnum"].max()),
            }
            pd.DataFrame([stats]).to_csv(out_dir / "paired_summary_pcnum_to_pccot.csv", index=False)

        # Optional plot: boxplot per prompt-category (per experiment)
        if do_plots:
            fig, ax = plt.subplots(figsize=(7.2, 4.0))
            data = [pd.to_numeric(sub_pcnum["loocv_r2"], errors="coerce").dropna(),
                    pd.to_numeric(sub_pccot["loocv_r2"], errors="coerce").dropna()]
            bp = ax.boxplot(data, tick_labels=["pcnum", "pccot"], patch_artist=True, showfliers=False)
            # Colors for boxes
            for i, box in enumerate(bp['boxes']):
                box.set_facecolor(numeric_color if i == 0 else cot_color)
                box.set_alpha(0.9)

            # Scatter all agents (circles)
            jitter = [-0.08, 0.08]
            for i, d in enumerate(data, start=1):
                x = np.full(len(d), i) + np.random.uniform(jitter[0], jitter[1], size=len(d))
                ax.scatter(x, d, alpha=0.6, s=22, edgecolors="none")  # circles

            # Medians (green triangles)
            med_vals = [np.median(d) if len(d) else np.nan for d in data]
            ax.scatter(np.array([1, 2], dtype=float), np.array(med_vals, dtype=float), marker="^", s=90, color="yellow", zorder=3)

            ax.set_ylabel(r"$\mathrm{LOOCV}\ R^2$")
            ax.set_title(f"{exp}: pcnum vs pccot")

            # Legend with proxy artists
            proxies = [
                Line2D([0], [0], marker='s', linestyle='none', markersize=10,
                       markerfacecolor="#cfe8ff", label="Distribution (box)"),
                Line2D([0], [0], marker='o', linestyle='none', markersize=6,
                       markerfacecolor="gray", alpha=0.6, label="Agent $R^2$ (circles)"),
                Line2D([0], [0], marker='^', linestyle='none', markersize=8,
                       markerfacecolor="yellow", label="Condition median (triangles)"),
            ]
            ax.legend(handles=proxies, loc="best", frameon=False, title="Legend")
            # Set legend to multiple columns for better readability
            ax.legend(handles=proxies, loc="best", frameon=False, title="Legend", ncol=2)
            fig.tight_layout()
            fig.savefig(out_dir / "box_R2_pcnum_vs_pccot.pdf")
            plt.close(fig)

        # Optional LaTeX single-row summary for manuscript tables
        if export_tex:
            tex_path = out_dir / "summary_pcnum_vs_pccot.tex"
            lines = []
            lines.append("% Auto-generated; do not edit.\n")
            lines.append("\\begin{tabular}{lrr}\n\\toprule\n")
            lines.append("Prompt & $\\tilde{R}^2$ & $N$ \\\\\n\\midrule\n")
            lines.append(f"pcnum & {med_pcnum:.3f} & {sub_pcnum['agent'].nunique()} \\\\\n")
            lines.append(f"pccot & {med_pccot:.3f} & {sub_pccot['agent'].nunique()} \\\\\n")
            lines.append("\\bottomrule\n\\end{tabular}\n")
            tex_path.write_text("".join(lines))
            print(f"Saved LaTeX: {tex_path}")




def matched_agents(master: pd.DataFrame,
                   expA: str, expB: str,
                   pc: str) -> pd.DataFrame:
    """Create a per-agent wide table for two experiments within one prompt-category.

    Parameters
    ----------
    master : DataFrame
        Master table (from build_master).
    expA, expB : str
        Baseline and comparison experiment names.
    pc : str
        Prompt-category (e.g., "pcnum" or "pccot").

    Returns
    -------
    pandas.DataFrame
        Wide table with metrics from expA (suffix __<expA>) and expB (suffix __<expB>)
        for agents present in both. Used to compute paired deltas and tests.

    What this reveals
    -----------------
    The matched set of agents across experiments ensures that any differences found
    are not driven by agent composition but reflect the experimental manipulation.
    """
    A = master[(master["experiment"] == expA) & (master["prompt_category"] == pc)].copy()
    B = master[(master["experiment"] == expB) & (master["prompt_category"] == pc)].copy()
    cols = ["loocv_r2", "b", "m1", "m2", "pC1", "pC2", "abs_mdiff", "params_tying"]
    A = A.set_index("agent")[cols]
    B = B.set_index("agent")[cols]
    common = A.index.intersection(B.index)
    if len(common) == 0:
        return pd.DataFrame()
    W = A.loc[common].add_suffix(f"__{expA}").join(B.loc[common].add_suffix(f"__{expB}"), how="inner")
    W["agent"] = W.index
    W.reset_index(drop=True, inplace=True)
    W["prompt_category"] = pc
    W["expA"] = expA
    W["expB"] = expB
    return W


def matched_pc(master: pd.DataFrame, exp: str) -> pd.DataFrame:
    """Create a per-agent wide table for pcnum vs pccot within the same experiment.

    Returns a wide dataframe with __pcnum and __pccot suffixes for each metric,
    only for agents that appear in both prompt-categories.
    """
    A = master[(master["experiment"] == exp) & (master["prompt_category"] == "pcnum")].copy()
    B = master[(master["experiment"] == exp) & (master["prompt_category"] == "pccot")].copy()
    cols = ["loocv_r2", "b", "m1", "m2", "pC1", "pC2", "abs_mdiff", "params_tying"]
    A = A.set_index("agent")[cols]
    B = B.set_index("agent")[cols]
    common = A.index.intersection(B.index)
    if len(common) == 0:
        return pd.DataFrame()
    W = A.loc[common].add_suffix("__pcnum").join(B.loc[common].add_suffix("__pccot"), how="inner")
    W["agent"] = W.index
    W.reset_index(drop=True, inplace=True)
    W["experiment"] = exp
    return W


def paired_wilcoxon(delta: pd.Series) -> Tuple[float, float]:
    """Paired Wilcoxon signed-rank test with a conservative fallback.

    If SciPy is unavailable, uses a two-sided sign test approximation (binomial)
    on the signs of non-zero deltas.
    """
    try:
        from scipy.stats import wilcoxon
        x = to_num(delta).dropna()
        if len(x) == 0:
            return (np.nan, np.nan)
        res = wilcoxon(x)
        # SciPy returns an object with attributes in newer versions; fallback to tuple indexing
        stat = float(getattr(res, "statistic", res[0]))  # type: ignore[index]
        p = float(getattr(res, "pvalue", res[1]))       # type: ignore[index]
        return stat, p
    except Exception:
        # Fallback: sign test (very conservative)
        x = to_num(delta).dropna()
        if len(x) == 0:
            return (np.nan, np.nan)
        n_pos = int((x > 0).sum())
        n_neg = int((x < 0).sum())
        n = n_pos + n_neg
        if n == 0:
            return (0.0, 1.0)
        # two-sided binomial exact under p=0.5
        from math import comb
        p_tail = sum(comb(n, k) for k in range(0, min(n_pos, n_neg) + 1)) / (2 ** n)
        return (float(n_pos - n_neg), float(2 * p_tail))


def paired_summary_table(W: pd.DataFrame,
                         lhs_suffix: str,
                         rhs_suffix: str,
                         label_cols: Dict[str, str]) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """Compute per-agent deltas and a paired-test summary for a matched wide table.

    Parameters
    ----------
    W : DataFrame
        Wide table from matched_agents or matched_pc. Must contain columns like
        "<metric>" + lhs_suffix and "<metric>" + rhs_suffix.
    lhs_suffix, rhs_suffix : str
        Suffixes denoting the left-hand side (A) and right-hand side (B) groups,
        e.g., "__rw17_indep_causes" and "__random_abstract" or "__pcnum"/"__pccot".
    label_cols : dict
        Mapping of output label columns to source columns in W to carry context
        (e.g., {"baseline": "expA", "other": "expB"}).

    Returns
    -------
    (DataFrame, DataFrame)
        Deltas per agent (one row per agent), and a test summary with Wilcoxon stats
        and BH–FDR adjusted p-values.

    Outputs (caller responsibility)
    -------------------------------
    Callers typically persist these to CSV, e.g., paired_deltas.csv and paired_summary.csv.

    What this reveals
    -----------------
    - deltas_per_agent: raw shifts in metrics per agent (B - A), useful for inspecting
      heterogeneity.
    - test_summary: median delta and paired test p-values per metric, indicating
      whether changes are systematic across agents.
    """
    metrics = ["loocv_r2", "b", "m1", "m2", "pC1", "pC2", "abs_mdiff"]
    deltas: Dict[str, pd.Series] = {}
    for m in metrics:
        a = to_num(W[f"{m}{lhs_suffix}"])
        b = to_num(W[f"{m}{rhs_suffix}"])
        deltas[f"delta_{m}"] = b - a

    # Include 3-param winner delta (binary to {-1,0,1})
    if f"params_tying{lhs_suffix}" in W.columns and f"params_tying{rhs_suffix}" in W.columns:
        lhs3 = (to_num(W[f"params_tying{lhs_suffix}"]) == 3).astype(float)
        rhs3 = (to_num(W[f"params_tying{rhs_suffix}"]) == 3).astype(float)
        deltas["delta_three_param_share"] = rhs3 - lhs3

    D = pd.DataFrame({"agent": W["agent"], **deltas})
    # Add identifying cols
    for out_name, src_col in label_cols.items():
        D[out_name] = W[src_col]

    # Wilcoxon + effect sizes (Hodges–Lehmann)
    rows = []
    for k, s in deltas.items():
        x = to_num(s).dropna()
        if x.empty:
            rows.append({"metric": k, "N": 0, "median_delta": np.nan, "wilcoxon_stat": np.nan,
                         "p_value": np.nan})
            continue
        stat, p = paired_wilcoxon(x)
        # Hodges–Lehmann (median of all pairwise averages for signed deltas == just median)
        med = float(np.median(x))
        rows.append({"metric": k, "N": int(len(x)), "median_delta": med,
                     "wilcoxon_stat": stat, "p_value": p})
    T = pd.DataFrame(rows)
    # BH-FDR adjust within this comparison
    rej, adj = bh_fdr(T["p_value"].fillna(1.0).values)
    T["p_fdr"] = adj
    T["reject_fdr"] = rej
    # Carry labels on the summary too
    for out_name, src_col in label_cols.items():
        T[out_name] = W[src_col].iloc[0] if len(W) else None
    return D, T


# ------------------------- Mixed-effects models -------------------------

def fit_mixed_R2(master: pd.DataFrame,
                 experiments: List[str],
                 include_unknown_pc: bool = False) -> pd.DataFrame:
    """Mixed-effects (or OLS-FE fallback) model for LOOCV R^2 across conditions.

    Model
    -----
    loocv_r2 ~ C(experiment) + C(prompt_category) + C(experiment):C(prompt_category)
               + (1 | agent)

    If MixedLM fails or statsmodels is unavailable, falls back to OLS with agent
    fixed effects and cluster-robust SE clustered by agent, then suppresses the
    many agent dummy rows in the returned table.

    Returns
    -------
    pandas.DataFrame
        Coefficient table with terms, estimates, SE, and p-values (CSV/LaTeX saved by caller).

    What this reveals
    -----------------
    Pooled, population-level contrasts between experiments and prompt-categories
    that account for unbalanced agent presence and within-agent correlation.
    """
    df = master.copy()
    if not include_unknown_pc:
        df = df[df["prompt_category"].isin(["pcnum", "pccot"])]

    # Make factors
    df["experiment"] = df["experiment"].astype("category")
    df["prompt_category"] = df["prompt_category"].astype("category")
    df = df.dropna(subset=["loocv_r2"])

    if HAS_SM:
        # Mixed effects: Gaussian
        # Use treatment coding with the first level as baseline (sorted order)
        formula = "loocv_r2 ~ C(experiment) + C(prompt_category) + C(experiment):C(prompt_category)"
        try:
            md = smf.mixedlm(formula, data=df, groups=df["agent"])  # type: ignore[call-arg]
            m = md.fit(reml=False, method="lbfgs")
            coefs = m.summary().tables[1]  # params
            tab = coefs.reset_index()
            tab.columns = ["term", "coef", "std_err", "z", "p_value", "[0.025", "0.975]"]
            tab["model"] = "MixedLM"
            return tab
        except Exception:
            pass

    # Fallback: OLS + agent fixed effects + cluster-robust SE (cluster by agent)
    # Build design with agent FE
    formula = "loocv_r2 ~ C(experiment) + C(prompt_category) + C(experiment):C(prompt_category) + C(agent)"
    ols = smf.ols(formula, data=df).fit(cov_type="cluster", cov_kwds={"groups": df["agent"]}) if HAS_SM else None
    if ols is not None:
        summ = ols.summary2().tables[1].reset_index()
        summ = summ.rename(columns={"index": "term", "Coef.": "coef", "Std.Err.": "std_err", "P>|t|": "p_value"})
        summ["model"] = "OLS_FE_cluster(agent)"
        # Filter out the many agent dummies when reporting
        return summ[~summ["term"].str.startswith("C(agent)")].reset_index(drop=True)
    # last-resort: empty frame
    return pd.DataFrame(columns=["term", "coef", "std_err", "p_value", "model"])


def fit_glm_three_param(master: pd.DataFrame) -> pd.DataFrame:
    """GLM Binomial for the probability that the 3-parameter model is the winner.

    Model
    -----
    I(params_tying==3) ~ C(experiment) + C(prompt_category) + C(experiment):C(prompt_category) + C(agent)

    Uses cluster-robust SE clustered by agent. Requires statsmodels.

    Returns
    -------
    pandas.DataFrame
        Coefficient table with terms, estimates, SE, and p-values (CSV/LaTeX saved by caller).

    What this reveals
    -----------------
    Which experiments/prompt-categories are associated with a higher share of simpler
    (3-parameter) winners, controlling for agent effects.
    """
    if not HAS_SM:
        return pd.DataFrame(columns=["term", "coef", "std_err", "p_value", "model"])

    df = master.copy()
    df = df.dropna(subset=["params_tying"])
    df["y"] = (to_num(df["params_tying"]) == 3).astype(int)
    df["experiment"] = df["experiment"].astype("category")
    df["prompt_category"] = df["prompt_category"].astype("category")

    formula = "y ~ C(experiment) + C(prompt_category) + C(experiment):C(prompt_category) + C(agent)"
    glm = smf.glm(formula, data=df, family=sm.families.Binomial()).fit(cov_type="cluster", cov_kwds={"groups": df["agent"]})
    summ = glm.summary2().tables[1].reset_index()
    summ = summ.rename(columns={"index": "term", "Coef.": "coef", "Std.Err.": "std_err", "P>|z|": "p_value"})
    summ["model"] = "GLM_Binomial_FE_cluster(agent)"
    return summ[~summ["term"].str.startswith("C(agent)")].reset_index(drop=True)


# ------------------------- Plots -------------------------



def plot_box_R2_by_exp_pc(master: pd.DataFrame, out_path: Path) -> None:
    """Grouped boxplot of LOOCV R^2 per experiment x prompt_category with legend.
    Shows pcnum (numeric) and pccot (CoT) side-by-side for each experiment.
    """
    df = master.copy()
    df = df[df["prompt_category"].isin(["pcnum", "pccot"])]
    if df.empty:
        return
    exps = sorted(df["experiment"].unique().tolist(), reverse=True)
    num_data, cot_data = [], []
    for e in exps:
        e_sub = df[df["experiment"] == e]
        num_data.append(to_num(e_sub[e_sub["prompt_category"]=="pcnum"]["loocv_r2"]).dropna().values)
        cot_data.append(to_num(e_sub[e_sub["prompt_category"]=="pccot"]["loocv_r2"]).dropna().values)

    fig, ax = plt.subplots()

    # positions: grouped by experiment, adjust width between experiment box-plot pairs
    group_spacing = 1.20         # 1.20  --> 20% more horizontal distance between experiment groups

    base = np.arange(len(exps)) * group_spacing    # centers at 0,1,2,...
    pair_offset = 0.2                              # was 0.20
    positions_num = base - pair_offset
    positions_cot = base + pair_offset
    # positions_num = np.arange(len(exps)) - 0.2
    # positions_cot = np.arange(len(exps)) + 0.2

    # bp1 = ax.boxplot(num_data, positions=positions_num, widths=0.35, 
    #                  patch_artist=True, showmeans=True, meanprops=dict(marker='^', markerfacecolor='green'))
    # bp2 = ax.boxplot(cot_data, positions=positions_cot, widths=0.35, 
    #                  patch_artist=True, showmeans=True, meanprops=dict(marker='^', markerfacecolor='green'))
    bp1 = ax.boxplot(num_data, positions=positions_num, widths=0.26, 
                     patch_artist=True, showmeans=True, meanprops=dict(marker='^', markerfacecolor='yellow'))
    bp2 = ax.boxplot(cot_data, positions=positions_cot, widths=0.26, 
                     patch_artist=True, showmeans=True, meanprops=dict(marker='^', markerfacecolor='yellow'))
    
    # --- token annotations (floored) ---
    ymin, ymax = ax.get_ylim()
    ytext = ymax - 0.03*(ymax - ymin)   # put near the top, inside the axes
    ytext = ymin + 0.03*(ymax - ymin)   # put near the bottom, inside the axes


    # Alternative: left/right of the tick at the bottom
    ytext = ymin - 0.05*(ymax - ymin)               # below plot area

    # # Alternative: left/right of the tick at the bottom
    # ytext = ymin - 0.05*(ymax - ymin)               # below plot area
    # ax.set_ylim(ymin, ymax)                          # keep limits so text shows
    # ax.text(base[i] - pair_offset, ytext, f"{int(tok_num)}", ha="center", va="top", transform=ax.transData)
    # ax.text(base[i] + pair_offset, ytext, f"{int(tok_cot)}", ha="center", va="top", transform=ax.transData)

        

    # umeric_color = (0.85, 0.60, 0.55)  # #D9998C
    # cot_color     = (0.00, 0.20, 0.55)  # #00338C
    for i, exp in enumerate(exps):
        # numeric (pcnum) on the left box of the pair
        tok_num = avg_tokens_for(exp, "pcnum")
        if pd.notna(tok_num):
            ax.text(positions_num[i]-0.05, ytext, f"{int(tok_num)}",
                    ha="center", va="bottom", fontsize=9, color=PROMPT_CATEGORY_COLORS["numeric"],
                    bbox=dict(boxstyle="round,pad=0.2", facecolor="white", alpha=.5, lw=0))

        # CoT (pccot) on the right box of the pair
        tok_cot = avg_tokens_for(exp, "pccot")
        if pd.notna(tok_cot):
            ax.text(positions_cot[i]+0.05, ytext, f"{int(tok_cot)}",
                    ha="center", va="bottom", fontsize=9, color=PROMPT_CATEGORY_COLORS["CoT"],
                    bbox=dict(boxstyle="round,pad=0.2", facecolor="white", alpha=.5, lw=0))

  

    # Colors
    for patch in bp1['boxes']:
        patch.set_facecolor(numeric_color)
    for patch in bp2['boxes']:
        patch.set_facecolor(cot_color)

    # Medians
    for median in bp1['medians']:
        median.set_color('black')
    for median in bp2['medians']:
        median.set_color('black')

    # Axis labels
    ax.set_xticks(base)
    ax.set_xticklabels([str(exp_name_map.get(e, e) or "") for e in exps], rotation=25) # 45 works well for 6
    ax.set_xlabel('Experiment', labelpad=6)  # Move x-label further down, 24 works well for 6

    # Move xtick labels further down by adjusting bottom margin
    ax.tick_params(axis='x', pad=14)  # Increase padding below ticks

    ax.set_ylabel('LOOCV $R^2$')
    # ax.set_title('Normativity by Content x Prompt-category')
    ax.set_title('Reasoning Consistency by Experiment')


    # Legend handles
    legend_patches = [
        Patch(facecolor=numeric_color, label='Numeric'),
        Patch(facecolor=cot_color, label='CoT')
    ]
    import matplotlib.lines as mlines
    mean_marker = mlines.Line2D([], [], color='yellow', marker='^', linestyle='None', label='Mean')
    triangle_frame = mlines.Line2D([], [], color='green', marker='^', linestyle='None', markeredgecolor='green', markerfacecolor='yellow', markersize=10, label='Mean (green frame)')
    median_line = mlines.Line2D([], [], color='black', linestyle='-', label='Median')

    # Combine into one legend
    ax.legend(handles=legend_patches + [mean_marker, median_line],
              loc='best', frameon=True, ncol=2, title="Prompt Category")

    ax.grid(axis='y', linestyle=':', alpha=0.4)
    # --- CSV: per-group summary and outliers ---
    try:
        out_dir = out_path.parent if out_path.suffix.lower() == ".pdf" else Path(out_path).parent
        out_dir.mkdir(parents=True, exist_ok=True)
    except Exception:
        out_dir = Path(".")

    rows = []
    outlier_rows = []
    for i, e in enumerate(exps):
        e_sub = df[df["experiment"] == e]
        for pc, series in ("pcnum", e_sub[e_sub["prompt_category"] == "pcnum"]["loocv_r2"]), ("pccot", e_sub[e_sub["prompt_category"] == "pccot"]["loocv_r2"]):
            s = pd.to_numeric(series, errors="coerce").dropna()
            summ = _iqr_summary(s)
            rows.append({
                "experiment": e,
                "prompt_category": pc,
                **summ,
            })
            # outliers beyond 1.5*IQR whiskers
            lf, uf = summ["lower_fence"], summ["upper_fence"]
            sub_agents = e_sub[e_sub["prompt_category"] == pc][["agent", "loocv_r2"]].copy()
            sub_agents["loocv_r2"] = pd.to_numeric(sub_agents["loocv_r2"], errors="coerce")
            mask = (sub_agents["loocv_r2"] < lf) | (sub_agents["loocv_r2"] > uf)
            for _, r in sub_agents[mask].iterrows():
                outlier_rows.append({
                    "experiment": e,
                    "prompt_category": pc,
                    "agent": r["agent"],
                    "loocv_r2": float(r["loocv_r2"]),
                    "lower_fence": lf,
                    "upper_fence": uf,
                })

    pd.DataFrame(rows).to_csv(out_dir / "box_R2_by_exp_pc_summary.csv", index=False)
    pd.DataFrame(outlier_rows).to_csv(out_dir / "box_R2_by_exp_pc_outliers.csv", index=False)

    fig.tight_layout()
    fig.savefig(out_path, dpi=200)
    plt.close(fig)



# Older but more labeled outliers version
def plot_box_R2_by_experiment_and_pc(master: pd.DataFrame, out_dest: Path,
                                     highlight_max_legend_agents: int = 12) -> None:
    """
    Overview plot: R^2 by experiment×prompt_category. Highlights agents in top/bottom quartiles
    within each group (experiment×pc). Only those agents get their own colored dots and legend
    entries (capped). Others are gray. Adds legend entries for triangles (condition medians),
    the black line (median inside box), and whiskers (1.5×IQR).

    out_dest can be a directory or a full filepath.
    """
    # Normalize output path
    out_dest = Path(out_dest)
    if out_dest.suffix.lower() == ".pdf":
        out_path = out_dest
        out_dir = out_dest.parent
    else:
        out_dir = out_dest
        out_dir.mkdir(parents=True, exist_ok=True)
        out_path = out_dir / "_box_R2_by_experiment_and_pc.pdf"
    out_dir.mkdir(parents=True, exist_ok=True)

    # Prep data
    tmp = (master.dropna(subset=["prompt_category"])
                 .assign(prompt_category=lambda d: d["prompt_category"].str.lower()))
    groups = tmp.groupby(["experiment", "prompt_category"])
    keys: List[Tuple[str, str]] = []
    for name in groups.groups.keys():
        if isinstance(name, tuple) and len(name) == 2:
            keys.append((str(name[0]), str(name[1])))
    keys.sort(key=lambda k: (k[0], "0" if k[1] == "pcnum" else "1"))

    # Build per-group data
    data, positions, tick_labels, per_group_df = [], [], [], []
    x = 1
    for (exp, pc) in keys:
        g = groups.get_group((exp, pc)).copy()
        vals = pd.to_numeric(g["loocv_r2"], errors="coerce").dropna()
        if vals.empty:
            continue
        data.append(vals.values)
        positions.append(x)
        tick_labels.append(f"{exp}\n{pc}")
        per_group_df.append(g)
        x += 1

    if not data:
        print("[WARN] No data to plot in box_R2_by_experiment_and_pc.")
        return

    # Determine highlighted agents across all groups (union) and assign colors
    highlighted_agents_all: list[str] = []
    for g in per_group_df:
        bottom_set, top_set, legend_bottom, legend_top = _top_bottom_agents_for_baseline(
            g[["agent", "loocv_r2"]], highlight_max_legend_agents
        )
        highlighted_agents_all.extend(legend_bottom + legend_top)
    # Unique preserve order
    seen = set()
    highlighted_agents_all = [a for a in highlighted_agents_all if not (a in seen or seen.add(a))]
    cmap = _agent_color_map(highlighted_agents_all)

    # Plot
    fig, ax = plt.subplots(figsize=(10.8, 5.8))
    bp = ax.boxplot(
        data, positions=positions, tick_labels=tick_labels,
        patch_artist=True, showfliers=False
    )

    # Style boxes (pcnum vs pccot)
    for i, box in enumerate(bp['boxes']):
        is_pcnum = "pcnum" in tick_labels[i].split("\n")[1]
        box.set_facecolor(numeric_color if is_pcnum else cot_color)
        box.set_alpha(0.9)
        box.set_edgecolor("black")

    # Style whiskers and medians (black line inside the box is median by default)
    for med in bp['medians']:
        med.set_color("black")
        med.set_linewidth(1.2)
    for w in bp['whiskers']:
        w.set_color("black")
        w.set_linewidth(1.0)
    for cap in bp['caps']:
        cap.set_color("black")
        cap.set_linewidth(1.0)

    # Scatter agents: highlighted get colors; others gray; add per-group medians (triangles)
    for i, g in enumerate(per_group_df):
        xpos = positions[i]
        vals = pd.to_numeric(g["loocv_r2"], errors="coerce")
        agents = g["agent"].astype(str).tolist()

        # quartiles for this group
        bottom_set, top_set, legend_bottom, legend_top = _top_bottom_agents_for_baseline(
            g[["agent", "loocv_r2"]], highlight_max_legend_agents
        )
        highlighted_here = set(legend_bottom + legend_top)

        # plot all points with jitter
        rng = np.random.default_rng(1337 + i)
        xj = rng.uniform(-0.08, 0.08, size=len(vals))
        for a, y, dx in zip(agents, vals, xj):
            if np.isnan(y):
                continue
            if a in highlighted_agents_all and a in highlighted_here:
                ax.scatter([xpos + dx], [y], s=26, color=cmap[a], zorder=3)
            else:
                ax.scatter([xpos + dx], [y], s=18, color="gray", alpha=0.6, edgecolors="none", zorder=2)

        # condition median (green triangle)
        medv = float(np.nanmedian(vals))
        ax.scatter([xpos], [medv], marker="^", s=90, color="yellow", zorder=4)
        ax.set_ylabel(r"$\mathrm{LOOCV}\ R^2$")
        ax.set_xlabel("Experiment and Prompt Category")
        
        # ax.set_title("Normativity by experiment x prompt category")
        ax.set_title('Reasoning Consistency by Experiment')


        # Rotate x tick labels for better readability
        for label in ax.get_xticklabels():
            label.set_rotation(25)  # or any angle you prefer

        # Build legend: structure proxies + highlighted agents (capped already by per-group limits)
        handles = _boxplot_legend_handles()
        # Add agent entries (only those in highlighted_agents_all, capped already by per-group limits)
        for a in highlighted_agents_all:
            handles.append(Line2D([0], [0], marker='o', linestyle='none', markersize=7,
                                  markerfacecolor=cmap[a], label=a))
        ax.legend(handles=handles, loc="best", frameon=False, title="Highlighted (top/bottom quartiles)", ncol=2)

        # CSV next to this plot: per (experiment,prompt) med/IQR + outliers
        rows = []
        outlier_rows = []
        for (exp, pc), g in tmp.groupby(["experiment", "prompt_category"], dropna=False):
            vals = pd.to_numeric(g["loocv_r2"], errors="coerce").dropna()
            summ = _iqr_summary(vals)
            rows.append({"experiment": exp, "prompt_category": pc, **summ})
            lf, uf = summ["lower_fence"], summ["upper_fence"]
            ag = g[["agent", "loocv_r2"]].copy()
            ag["loocv_r2"] = pd.to_numeric(ag["loocv_r2"], errors="coerce")
            mask = (ag["loocv_r2"] < lf) | (ag["loocv_r2"] > uf)
            for _, r in ag[mask].iterrows():
                outlier_rows.append({
                    "experiment": exp,
                    "prompt_category": pc,
                    "agent": r["agent"],
                    "loocv_r2": float(r["loocv_r2"]),
                    "lower_fence": lf,
                    "upper_fence": uf,
                })

        pd.DataFrame(rows).to_csv(out_dir / "box_R2_by_experiment_and_pc_summary.csv", index=False)
        pd.DataFrame(outlier_rows).to_csv(out_dir / "box_R2_by_experiment_and_pc_outliers.csv", index=False)

        fig.tight_layout()
        fig.savefig(out_path, dpi=200)
        plt.close(fig)



# Older but more labeled outliers version
def plot_box_R2_by_experiment_and_pc(master: pd.DataFrame, out_dest: Path,
                                     highlight_max_legend_agents: int = 12) -> None:
    """
    Overview plot: R^2 by experiment×prompt_category. Highlights agents in top/bottom quartiles
    within each group (experiment×pc). Only those agents get their own colored dots and legend
    entries (capped). Others are gray. Adds legend entries for triangles (condition medians),
    the black line (median inside box), and whiskers (1.5×IQR).

    out_dest can be a directory or a full filepath.
    """
    # Normalize output path
    out_dest = Path(out_dest)
    if out_dest.suffix.lower() == ".pdf":
        out_path = out_dest
        out_dir = out_dest.parent
    else:
        out_dir = out_dest
        out_dir.mkdir(parents=True, exist_ok=True)
        out_path = out_dir / "_box_R2_by_experiment_and_pc.pdf"
    out_dir.mkdir(parents=True, exist_ok=True)

    # Prep data
    tmp = (master.dropna(subset=["prompt_category"])
                 .assign(prompt_category=lambda d: d["prompt_category"].str.lower()))
    groups = tmp.groupby(["experiment", "prompt_category"])
    keys: List[Tuple[str, str]] = []
    for name in groups.groups.keys():
        if isinstance(name, tuple) and len(name) == 2:
            keys.append((str(name[0]), str(name[1])))
    keys.sort(key=lambda k: (k[0], "0" if k[1] == "pcnum" else "1"))

    # Build per-group data
    data, positions, tick_labels, per_group_df = [], [], [], []
    x = 1
    for (exp, pc) in keys:
        g = groups.get_group((exp, pc)).copy()
        vals = pd.to_numeric(g["loocv_r2"], errors="coerce").dropna()
        if vals.empty:
            continue
        data.append(vals.values)
        positions.append(x)
        tick_labels.append(f"{exp}\n{pc}")
        per_group_df.append(g)
        x += 1

    if not data:
        print("[WARN] No data to plot in box_R2_by_experiment_and_pc.")
        return

    # Determine highlighted agents across all groups (union) and assign colors
    highlighted_agents_all: list[str] = []
    for g in per_group_df:
        bottom_set, top_set, legend_bottom, legend_top = _top_bottom_agents_for_baseline(
            g[["agent", "loocv_r2"]], highlight_max_legend_agents
        )
        highlighted_agents_all.extend(legend_bottom + legend_top)
    # Unique preserve order
    seen = set()
    highlighted_agents_all = [a for a in highlighted_agents_all if not (a in seen or seen.add(a))]
    cmap = _agent_color_map(highlighted_agents_all)

    # Plot
    fig, ax = plt.subplots(figsize=(10.8, 5.8))
    bp = ax.boxplot(
        data, positions=positions, tick_labels=tick_labels,
        patch_artist=True, showfliers=False
    )

    # Style boxes (pcnum vs pccot)
    for i, box in enumerate(bp['boxes']):
        is_pcnum = "pcnum" in tick_labels[i].split("\n")[1]
        box.set_facecolor(numeric_color if is_pcnum else cot_color)
        box.set_alpha(0.9)
        box.set_edgecolor("black")

    # Style whiskers and medians (black line inside the box is median by default)
    for med in bp['medians']:
        med.set_color("black")
        med.set_linewidth(1.2)
    for w in bp['whiskers']:
        w.set_color("black")
        w.set_linewidth(1.0)
    for cap in bp['caps']:
        cap.set_color("black")
        cap.set_linewidth(1.0)

    # Scatter agents: highlighted get colors; others gray; add per-group medians (triangles)
    for i, g in enumerate(per_group_df):
        xpos = positions[i]
        vals = pd.to_numeric(g["loocv_r2"], errors="coerce")
        agents = g["agent"].astype(str).tolist()

        # quartiles for this group
        bottom_set, top_set, legend_bottom, legend_top = _top_bottom_agents_for_baseline(
            g[["agent", "loocv_r2"]], highlight_max_legend_agents
        )
        highlighted_here = set(legend_bottom + legend_top)

        # plot all points with jitter
        rng = np.random.default_rng(1337 + i)
        xj = rng.uniform(-0.08, 0.08, size=len(vals))
        for a, y, dx in zip(agents, vals, xj):
            if np.isnan(y):
                continue
            if a in highlighted_agents_all and a in highlighted_here:
                ax.scatter([xpos + dx], [y], s=26, color=cmap[a], zorder=3)
            else:
                ax.scatter([xpos + dx], [y], s=18, color="gray", alpha=0.6, edgecolors="none", zorder=2)

        # condition median (green triangle)
        medv = float(np.nanmedian(vals))
        ax.scatter([xpos], [medv], marker="^", s=90, color="yellow", zorder=4)
        ax.set_ylabel(r"$\mathrm{LOOCV}\ R^2$")
        ax.set_xlabel("Experiment and Prompt Category")
        
        # ax.set_title("Normativity by experiment x prompt category")
        ax.set_title('Reasoning Consistency by Experiment')


        # Rotate x tick labels for better readability
        for label in ax.get_xticklabels():
            label.set_rotation(25)  # or any angle you prefer

        # Build legend: structure proxies + highlighted agents (capped already by per-group limits)
        handles = _boxplot_legend_handles()
        # Add agent entries (only those in highlighted_agents_all, capped already by per-group limits)
        for a in highlighted_agents_all:
            handles.append(Line2D([0], [0], marker='o', linestyle='none', markersize=7,
                                  markerfacecolor=cmap[a], label=a))
        ax.legend(handles=handles, loc="best", frameon=False, title="Highlighted (top/bottom quartiles)", ncol=2)

        # CSV next to this plot: per (experiment,prompt) med/IQR + outliers
        rows = []
        outlier_rows = []
        for (exp, pc), g in tmp.groupby(["experiment", "prompt_category"], dropna=False):
            vals = pd.to_numeric(g["loocv_r2"], errors="coerce").dropna()
            summ = _iqr_summary(vals)
            rows.append({"experiment": exp, "prompt_category": pc, **summ})
            lf, uf = summ["lower_fence"], summ["upper_fence"]
            ag = g[["agent", "loocv_r2"]].copy()
            ag["loocv_r2"] = pd.to_numeric(ag["loocv_r2"], errors="coerce")
            mask = (ag["loocv_r2"] < lf) | (ag["loocv_r2"] > uf)
            for _, r in ag[mask].iterrows():
                outlier_rows.append({
                    "experiment": exp,
                    "prompt_category": pc,
                    "agent": r["agent"],
                    "loocv_r2": float(r["loocv_r2"]),
                    "lower_fence": lf,
                    "upper_fence": uf,
                })

        pd.DataFrame(rows).to_csv(out_dir / "box_R2_by_experiment_and_pc_summary.csv", index=False)
        pd.DataFrame(outlier_rows).to_csv(out_dir / "box_R2_by_experiment_and_pc_outliers.csv", index=False)

        fig.tight_layout()
        fig.savefig(out_path, dpi=200)
        plt.close(fig)




def plot_slope_R2_pair(
    merged: pd.DataFrame,
    labelA: str,
    labelB: str,
    out_path: Path,
    *,
    colA: Optional[str] = None,
    colB: Optional[str] = None,
    highlight_max_legend_agents: int = 12,
) -> None:
    """
    Per-agent slope from A->B with highlighted agents (top/bottom quartiles on A).
    You MUST pass colA/colB (the LOOCV R^2 columns to plot), e.g.:
      - colA="loocv_r2__rw17_indep_causes", colB="loocv_r2__random_abstract"
      - colA="loocv_r2__pcnum",            colB="loocv_r2__pccot"
    """
    if merged.empty:
        print(f"[WARN] slope plot skipped; no pairs for {labelA} vs {labelB}.")
        return

    # ---- resolve columns robustly ----
    cols = list(merged.columns)
    # If explicit cols not provided, try common fallbacks
    if colA is None or colB is None:
        candidates = [c for c in cols if c.startswith("loocv_r2")]
        if set(("loocv_r2_A", "loocv_r2_B")).issubset(cols):
            colA = colA or "loocv_r2_A"
            colB = colB or "loocv_r2_B"
        elif len(candidates) == 2:
            # arbitrary but stable order
            candidates.sort()
            colA = colA or candidates[0]
            colB = colB or candidates[1]
        else:
            raise ValueError(
                f"plot_slope_R2_pair cannot infer columns. "
                f"Pass colA/colB explicitly. Available: {candidates}"
            )
    if colA not in cols or colB not in cols:
        raise KeyError(f"Required columns not found: colA={colA!r}, colB={colB!r}; available={cols}")

    # ---- numeric conversion, drop missing ----
    X = merged.copy()
    X[colA] = pd.to_numeric(X[colA], errors="coerce")
    X[colB] = pd.to_numeric(X[colB], errors="coerce")
    X = X.dropna(subset=[colA, colB, "agent"])
    if X.empty:
        print(f"[WARN] slope plot skipped after NA drop: {labelA} vs {labelB}.")
        return

    # ---- compute highlight sets on baseline (A) ----
    df_base = X[["agent", colA]].rename(columns={colA: "loocv_r2"})
    _, _, legend_bottom, legend_top = _top_bottom_agents_for_baseline(
        df_base, highlight_max_legend_agents
    )
    special_agents = legend_bottom + legend_top
    cmap = _agent_color_map(special_agents)

    # ---- draw ----
    fig, ax = plt.subplots(figsize=(7.2, 4.6))
    xA, xB = 0, 1

    for _, r in X.iterrows():
        a = str(r["agent"])
        yA = float(r[colA])
        yB = float(r[colB])
        color = cmap.get(a, (0.7, 0.7, 0.7))
        lw = 1.8 if a in cmap else 1.0
        alpha = 0.9 if a in cmap else 0.6
        ax.plot([xA, xB], [yA, yB], color=color, lw=lw, alpha=alpha)
        if a in cmap:
            ax.scatter([xA, xB], [yA, yB], s=28, color=color, zorder=3)

    # median overlay in red
    medA = float(np.nanmedian(pd.to_numeric(X[colA], errors="coerce").to_numpy()))
    medB = float(np.nanmedian(pd.to_numeric(X[colB], errors="coerce").to_numpy()))
    ax.plot([xA, xB], [medA, medB], color="red", lw=3.0, alpha=0.9, label="Median change")

    # axes/legend
    ax.set_xticks([xA, xB], [labelA, labelB])
    ax.set_ylabel(r"$\mathrm{LOOCV}\ R^2$")
    n = X["agent"].nunique()
    ax.set_title(f"Slope of $R^2$ ({labelA} → {labelB}); $N={n}$ agents")

    handles = [Line2D([0], [0], color="red", lw=3.0, label="Median change")]
    for a in special_agents:
        handles.append(Line2D([0], [0], color=cmap[a], lw=2.0, label=a))
    if len(handles) > 1:
        ax.legend(handles=handles, loc="best", frameon=False, title="Highlighted (top/bottom quartiles)", ncol=2)

    ax.grid(True, axis="y", alpha=0.2)
    fig.tight_layout()
    out_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(out_path)
    plt.close(fig)
    print(f"Saved plot: {out_path}")



def plot_slope_R2_pair(
    merged: pd.DataFrame,
    labelA: str,
    labelB: str,
    out_path: Path,
    *,
    colA: Optional[str] = None,
    colB: Optional[str] = None,
    highlight_max_legend_agents: int = 12,
) -> None:
    """
    Per-agent slope from A->B with highlighted agents (top/bottom quartiles on A).
    You MUST pass colA/colB (the LOOCV R^2 columns to plot), e.g.:
      - colA="loocv_r2__rw17_indep_causes", colB="loocv_r2__random_abstract"
      - colA="loocv_r2__pcnum",            colB="loocv_r2__pccot"
    """
    if merged.empty:
        print(f"[WARN] slope plot skipped; no pairs for {labelA} vs {labelB}.")
        return

    # ---- resolve columns robustly ----
    cols = list(merged.columns)
    # If explicit cols not provided, try common fallbacks
    if colA is None or colB is None:
        candidates = [c for c in cols if c.startswith("loocv_r2")]
        if set(("loocv_r2_A", "loocv_r2_B")).issubset(cols):
            colA = colA or "loocv_r2_A"
            colB = colB or "loocv_r2_B"
        elif len(candidates) == 2:
            # arbitrary but stable order
            candidates.sort()
            colA = colA or candidates[0]
            colB = colB or candidates[1]
        else:
            raise ValueError(
                f"plot_slope_R2_pair cannot infer columns. "
                f"Pass colA/colB explicitly. Available: {candidates}"
            )
    if colA not in cols or colB not in cols:
        raise KeyError(f"Required columns not found: colA={colA!r}, colB={colB!r}; available={cols}")

    # ---- numeric conversion, drop missing ----
    X = merged.copy()
    X[colA] = pd.to_numeric(X[colA], errors="coerce")
    X[colB] = pd.to_numeric(X[colB], errors="coerce")
    X = X.dropna(subset=[colA, colB, "agent"])
    if X.empty:
        print(f"[WARN] slope plot skipped after NA drop: {labelA} vs {labelB}.")
        return

    # ---- compute highlight sets on baseline (A) ----
    df_base = X[["agent", colA]].rename(columns={colA: "loocv_r2"})
    _, _, legend_bottom, legend_top = _top_bottom_agents_for_baseline(
        df_base, highlight_max_legend_agents
    )
    special_agents = legend_bottom + legend_top
    cmap = _agent_color_map(special_agents)

    # ---- draw ----
    fig, ax = plt.subplots(figsize=(7.2, 4.6))
    xA, xB = 0, 1

    for _, r in X.iterrows():
        a = str(r["agent"])
        yA = float(r[colA])
        yB = float(r[colB])
        color = cmap.get(a, (0.7, 0.7, 0.7))
        lw = 1.8 if a in cmap else 1.0
        alpha = 0.9 if a in cmap else 0.6
        ax.plot([xA, xB], [yA, yB], color=color, lw=lw, alpha=alpha)
        if a in cmap:
            ax.scatter([xA, xB], [yA, yB], s=28, color=color, zorder=3)

    # median overlay in red
    medA = float(np.nanmedian(pd.to_numeric(X[colA], errors="coerce").to_numpy()))
    medB = float(np.nanmedian(pd.to_numeric(X[colB], errors="coerce").to_numpy()))
    ax.plot([xA, xB], [medA, medB], color="red", lw=3.0, alpha=0.9, label="Median change")

    # axes/legend
    ax.set_xticks([xA, xB], [labelA, labelB])
    ax.set_ylabel(r"$\mathrm{LOOCV}\ R^2$")
    n = X["agent"].nunique()
    ax.set_title(f"Slope of $R^2$ ({labelA} → {labelB}); $N={n}$ agents")

    handles = [Line2D([0], [0], color="red", lw=3.0, label="Median change")]
    for a in special_agents:
        handles.append(Line2D([0], [0], color=cmap[a], lw=2.0, label=a))
    if len(handles) > 1:
        ax.legend(handles=handles, loc="best", frameon=False, title="Highlighted (top/bottom quartiles)", ncol=2)

    ax.grid(True, axis="y", alpha=0.2)
    fig.tight_layout()
    out_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(out_path)
    plt.close(fig)
    print(f"Saved plot: {out_path}")


def plot_slope_params_exp_pair(
    W: pd.DataFrame,
    suffixA: str,
    suffixB: str,
    labelA: str,
    labelB: str,
    out_dir: Path,
    highlight_max_legend_agents: int = 12,
) -> None:
    """Paired slope plots for parameters comparing two experiments (A → B) within one prompt-category.

    W is the output of matched_agents(...), with columns like b__<expA>, b__<expB>, etc.
    suffixA/suffixB must include the leading double-underscore, e.g., "__rw17_indep_causes".
    """
    out_dir.mkdir(parents=True, exist_ok=True)
    if W.empty:
        return

    agents = W["agent"].astype(str)

    def _plot_pair(valsA: pd.Series, valsB: pd.Series, param: str):
        X = pd.DataFrame({
            "agent": agents,
            "A": pd.to_numeric(valsA, errors="coerce"),
            "B": pd.to_numeric(valsB, errors="coerce"),
        }).dropna(subset=["A", "B"])
        if X.empty:
            return

        base_df = X[["agent", "A"]].rename(columns={"A": "loocv_r2"})
        _, _, legend_bottom, legend_top = _top_bottom_agents_for_baseline(base_df, highlight_max_legend_agents)
        special = legend_bottom + legend_top
        cmap = _agent_color_map(special)

        fig, ax = plt.subplots(figsize=(7.2, 4.2))
        xA, xB = 0, 1
        for _, r in X.iterrows():
            a = str(r["agent"])
            yA = float(r["A"])
            yB = float(r["B"])
            color = cmap.get(a, (0.7, 0.7, 0.7))
            lw = 1.8 if a in cmap else 1.0
            alpha = 0.9 if a in cmap else 0.6
            ax.plot([xA, xB], [yA, yB], color=color, lw=lw, alpha=alpha)
            if a in cmap:
                ax.scatter([xA, xB], [yA, yB], s=26, color=color, zorder=3)

        medA = float(np.nanmedian(X["A"].to_numpy()))
        medB = float(np.nanmedian(X["B"].to_numpy()))
        ax.plot([xA, xB], [medA, medB], color="red", lw=3.0, alpha=0.9, label="Median change")

        ax.set_xticks([xA, xB], [labelA, labelB])
        ax.set_ylabel(_pretty_param_label(param))
        # parameters are probabilities per user note
        ax.set_ylim(0.0, 1.0)
        n = X["agent"].nunique()
        ax.set_title(f"{_pretty_param_label(param)}: {labelA} → {labelB}; N={n} agents")

        handles = [Line2D([0], [0], color="red", lw=3.0, label="Median change")]
        for a in special:
            handles.append(Line2D([0], [0], color=cmap[a], lw=2.0, label=a))
        if len(handles) > 1:
            ax.legend(handles=handles, loc="best", frameon=False, title="Highlighted", ncol=2)

        ax.grid(True, axis="y", alpha=0.2)
        fig.tight_layout()
        out_path = out_dir / f"slope_{param}_A_to_B.pdf"
        fig.savefig(out_path)
        plt.close(fig)

    for p in ["b", "m1", "m2"]:
        cA, cB = f"{p}{suffixA}", f"{p}{suffixB}"
        if cA in W.columns and cB in W.columns:
            _plot_pair(W[cA], W[cB], p)

    if {f"pC1{suffixA}", f"pC2{suffixA}"}.issubset(W.columns) and {f"pC1{suffixB}", f"pC2{suffixB}"}.issubset(W.columns):
        pC_A = (pd.to_numeric(W[f"pC1{suffixA}"], errors="coerce") + pd.to_numeric(W[f"pC2{suffixA}"], errors="coerce")) / 2.0
        pC_B = (pd.to_numeric(W[f"pC1{suffixB}"], errors="coerce") + pd.to_numeric(W[f"pC2{suffixB}"], errors="coerce")) / 2.0
        _plot_pair(pC_A, pC_B, "pCavg")


# ------------------------- Combined slope subplots -------------------------

# Layout knobs for combined-slope figures (centralized so you can tweak once):
# - SUBPLOT_H_PER_ROW: Increase this to make each subfigure (axes) TALLER.
#   For example, change 3.0 → 3.6 to add +0.6 inch per row of subplots.
# - SUBPLOT_W_PER_COL: Increase this to make each subfigure WIDER.
# - EXTRA_FIG_H / EXTRA_FIG_W: Extra inches added to the overall figure (legend, margins).
#
# In short: To make each subfigure taller without changing the grid shape, raise
# SUBPLOT_H_PER_ROW. If the legend gets cramped, also raise EXTRA_FIG_H a bit.
SUBPLOT_H_PER_ROW = 4.0  # inches per row of subplots (taller panels → increase this)
SUBPLOT_W_PER_COL = 2.7  # inches per column of subplots (wider panels → increase this)
EXTRA_FIG_H = 1.5        # extra vertical inches (legend, suptitle padding)
EXTRA_FIG_W = 0.8        # extra horizontal inches (side margins)

def _agent_handles(agents: List[str]) -> List[Line2D]:
    """Create legend handles for agents using the stable global color mapping."""
    handles: List[Line2D] = []
    for a in agents:
        handles.append(
            Line2D([0], [0], color=agent_color(a), lw=2.0, label=str(a))
        )
    return handles

########################################################
########################################################
########################################################
def plot_combined_slopes_pcnum_to_pccot(W: pd.DataFrame, out_path: Path) -> None:
    """One figure containing slope plots for R^2 and parameters (pcnum → pccot).

    Panels include: R^2, b, m1, m2, and pCavg (if available), sharing a single legend
    below the figure. Agent colors are stable across figures.
    """
    if W.empty:
        return

    # Determine available panels
    panels: List[Tuple[str, str, str]] = []  # (display_name, colA, colB)
    if {"loocv_r2__pcnum", "loocv_r2__pccot"}.issubset(W.columns):
        panels.append((r"$\mathrm{LOOCV}\ R^2$", "loocv_r2__pcnum", "loocv_r2__pccot"))
    for p in ["b", "m1", "m2"]:
        cA, cB = f"{p}__pcnum", f"{p}__pccot"
        if {cA, cB}.issubset(W.columns):
            panels.append((f"CBN-parameter {_pretty_param_label(p)}", cA, cB))
    has_pC = {"pC1__pcnum", "pC2__pcnum", "pC1__pccot", "pC2__pccot"}.issubset(W.columns)
    if has_pC:
        panels.append((f"CBN-parameter {_pretty_param_label('pCavg')}", "pCavg__pcnum", "pCavg__pccot"))

    if not panels:
        return

    # Build working copy with pC averages if needed
    X = W.copy()
    if has_pC:
        X["pCavg__pcnum"] = (to_num(X["pC1__pcnum"]) + to_num(X["pC2__pcnum"])) / 2.0
        X["pCavg__pccot"] = (to_num(X["pC1__pccot"]) + to_num(X["pC2__pccot"])) / 2.0

    agents = X["agent"].astype(str).dropna().unique().tolist()
    agents.sort()

    nP = len(panels)
    ncols = 3 if nP >= 3 else nP
    nrows = int(np.ceil(nP / ncols)) if nP else 1
    # Figure size controls the overall real estate (width, height) allocated to all subplots.
    # If the legend becomes very tall (many agents) or labels overlap, increase the base
    # multipliers below. Width ~ 2.7 per column; Height ~ 2.5 per row + extra headroom.
    # Knob: figsize -> first value increases horizontal space; second increases vertical space.
    # figsize = (width_inches, height_inches)
    # Height = SUBPLOT_H_PER_ROW * nrows + EXTRA_FIG_H
    # Increase SUBPLOT_H_PER_ROW to make every subplot taller (recommended).
    fig, axes = plt.subplots(
        nrows=nrows,
        ncols=ncols,
        figsize=(SUBPLOT_W_PER_COL * ncols + EXTRA_FIG_W, SUBPLOT_H_PER_ROW * nrows + EXTRA_FIG_H),
    )
    axes_arr = np.array(axes).reshape(nrows, ncols) if isinstance(axes, np.ndarray) else np.array([[axes]])

    used_positions: set[tuple[int, int]] = set()
    for i, (title, cA, cB) in enumerate(panels):
        # Place panels row-wise, but RIGHT-ALIGN the last row so the final panel
        # appears in the lower-right subplot (not lower-left).
        # This maps the last row columns to start at (ncols - rem) ... ncols-1.
        r, c0 = divmod(i, ncols)
        rem = nP - (nrows - 1) * ncols  # panels in the last row
        if nrows > 1 and r == (nrows - 1) and rem < ncols:
            c = (ncols - rem) + c0
        else:
            c = c0
        ax = axes_arr[r, c]
        # Drop NaNs
        dfp = X[["agent", cA, cB]].copy()
        dfp[cA] = to_num(dfp[cA])
        dfp[cB] = to_num(dfp[cB])
        dfp = dfp.dropna(subset=[cA, cB])
        if dfp.empty:
            ax.axis("off")
            continue
        for _, row in dfp.iterrows():
            a = str(row["agent"]) 
            y0 = float(row[cA])
            y1 = float(row[cB])
            st = agent_style(a)
            ax.plot([0, 1], [y0, y1], color=st["color"], lw=1.4, alpha=0.95, ls=st["linestyle"])
            ax.scatter([0, 1], [y0, y1], color=st["color"], s=10, zorder=3, marker=st["marker"])
        # Add median dashed black line
        medA = float(np.nanmedian(dfp[cA].to_numpy()))
        medB = float(np.nanmedian(dfp[cB].to_numpy()))
        ax.plot([0, 1], [medA, medB], color="black", lw=3.0, ls="--")
        ax.set_xticks([0, 1], [PC_PRINT_MAP.get("pcnum", "pcnum"), PC_PRINT_MAP.get("pccot", "pccot")])
        ax.set_title(title)
        if "CBN-parameter" in title:
            ax.set_ylim(0.0, 1.0)
        # Remove y-labels per request; titles already state the quantity
        ax.set_ylabel("")
        ax.grid(True, axis="y", alpha=0.25)
        used_positions.add((r, c))
    # no-op: legend is figure-level below; nothing to store per-axis

    # Hide any subplot that wasn't used (left-side blanks in the last row after right-align)
    for rr in range(nrows):
        for cc in range(ncols):
            if (rr, cc) not in used_positions:
                axes_arr[rr, cc].axis("off")

    # Build a single, figure-level legend (bottom-right) so it doesn't overlap a panel
    # and cannot be clipped by tight_layout. If the legend is still cramped for many
    # agents, either (a) increase figsize, or (b) raise the bottom margin in tight_layout
    # below (increase the bottom value in "rect=(left, bottom, right, top)").
    from matplotlib.lines import Line2D as _L2D
    median_handle = _L2D([0], [0], color="black", lw=2.5, ls="--", label="Median")
    agent_handles: List[_L2D] = []
    for a in agents:
        st = agent_style(a)
        agent_handles.append(
            _L2D([0], [0], color=st["color"], lw=2.0, ls=st["linestyle"], marker=st["marker"], label=str(a))
        )
    handles = [median_handle] + agent_handles
    # Knobs for legend placement/visibility:
    # - loc + bbox_to_anchor control the legend anchor in figure coordinates (0..1).
    #   Move x in bbox_to_anchor a bit left (<1.0) if it gets too close to the edge.
    # - ncol controls legend compactness; increase for wider, shorter legends.
    # fig.legend(
    #     handles=handles,
    #     loc="lower right",
    #     bbox_to_anchor=(0.99, 0.3),  # (x, y) in figure coords; raise y to move legend upward
    #     ncol=2,
    #     frameon=True,
    #     title="Agents",
    #     borderaxespad=0.5,
    #     columnspacing=0.7,
    #     handletextpad=0.6,
    # )

    fig.legend(
        handles=handles,
        loc="lower center",
        bbox_to_anchor=(0.5, 0.02),  # (x, y) in figure coords; raise y to move legend upward
        ncol=4,
        frameon=True,
        title="Agents",
        borderaxespad=0.5,
        columnspacing=0.7,
        handletextpad=0.6,
    )

    # Title
    try:
        exp_raw = str(W.get("experiment", pd.Series([None])).iloc[0])
    except Exception:
        exp_raw = ""
    exp_pretty = str(exp_name_map.get(exp_raw, exp_raw) or exp_raw)
    n_agents = int(W["agent"].nunique())
    # Reduce whitespace above the title by pushing the suptitle closer to the top.
    # Knob: suptitle y -> raise toward 1.00 to reduce white space above the title.
    fig.suptitle(f"Experiment {exp_pretty}: Numeric → CoT; N={n_agents} matched agents", y=0.985)

    out_path.parent.mkdir(parents=True, exist_ok=True)
    # Layout knobs for spacing and reserved margins:
    # - tight_layout(rect=(left, bottom, right, top)) reserves margins around subplots
    #   so titles/labels don't overlap the legend or the figure edge.
    # - Increase bottom if the bottom legend is cramped; increase top to fit a tall suptitle.
    # Shrink the top reserved margin for subplots (top closer to 1.0 = less space above axes).
    # Knob: rect top -> increase (e.g., 0.96–0.98) to reclaim vertical space at the top.
    fig.tight_layout(rect=(0.04, 0.16, 0.98, 0.96))
    # - subplots_adjust(wspace, hspace) increases the spacing BETWEEN subplots
    #   (wspace = horizontal gap, hspace = vertical gap). Raise these if axes still touch.
    # fig.subplots_adjust(wspace=0.18, hspace=0.28)
    # fig.subplots_adjust(wspace=0.2, hspace=0.3)
    fig.subplots_adjust(wspace=0.4, hspace=0.2)

    # Crop extra outer margins (including above the suptitle) while keeping a tiny padding.
    # Knob: pad_inches -> decrease to crop tighter; increase to add a bit more outer space.
    fig.savefig(out_path, bbox_inches="tight", pad_inches=0.03)
    plt.close(fig)
####################

def plot_combined_slopes_exp_pair(
    W: pd.DataFrame,
    labelA: str,
    labelB: str,
    suffixA: str,
    suffixB: str,
    out_path: Path,
) -> None:
    """One figure with slope plots for R^2 and parameters (A → B) within a prompt-category."""
    if W.empty:
        return

    panels: List[Tuple[str, str, str]] = []
    # R^2
    r2A, r2B = f"loocv_r2{suffixA}", f"loocv_r2{suffixB}"
    if {r2A, r2B}.issubset(W.columns):
        panels.append((r"$\mathrm{LOOCV}\ R^2$", r2A, r2B))
    # params
    for p in ["b", "m1", "m2"]:
        cA, cB = f"{p}{suffixA}", f"{p}{suffixB}"
        if {cA, cB}.issubset(W.columns):
            panels.append((f"CBN-parameter {_pretty_param_label(p)}", cA, cB))
    has_pC = {f"pC1{suffixA}", f"pC2{suffixA}", f"pC1{suffixB}", f"pC2{suffixB}"}.issubset(W.columns)
    if has_pC:
        panels.append((f"CBN-parameter {_pretty_param_label('pCavg')}", f"pCavg{suffixA}", f"pCavg{suffixB}"))
    if not panels:
        return

    X = W.copy()
    if has_pC:
        X[f"pCavg{suffixA}"] = (to_num(X[f"pC1{suffixA}"]) + to_num(X[f"pC2{suffixA}"])) / 2.0
        X[f"pCavg{suffixB}"] = (to_num(X[f"pC1{suffixB}"]) + to_num(X[f"pC2{suffixB}"])) / 2.0

    agents = X["agent"].astype(str).dropna().unique().tolist()
    agents.sort()

    nP = len(panels)
    ncols = 3 if nP >= 3 else nP
    nrows = int(np.ceil(nP / ncols)) if nP else 1
    # See notes in the pcnum→pccot variant: figsize is the primary knob to scale everything.
    # Same knobs apply here: raise SUBPLOT_H_PER_ROW to make each panel taller.
    fig, axes = plt.subplots(
        nrows=nrows,
        ncols=ncols,
        figsize=(SUBPLOT_W_PER_COL * ncols + EXTRA_FIG_W, SUBPLOT_H_PER_ROW * nrows + EXTRA_FIG_H),
    )
    axes_arr = np.array(axes).reshape(nrows, ncols) if isinstance(axes, np.ndarray) else np.array([[axes]])

    used_positions: set[tuple[int, int]] = set()
    for i, (title, cA, cB) in enumerate(panels):
        # Right-align the last row so the final panel renders in the lower-right slot.
        r, c0 = divmod(i, ncols)
        rem = nP - (nrows - 1) * ncols
        if nrows > 1 and r == (nrows - 1) and rem < ncols:
            c = (ncols - rem) + c0
        else:
            c = c0
        ax = axes_arr[r, c]
        dfp = X[["agent", cA, cB]].copy()
        dfp[cA] = to_num(dfp[cA])
        dfp[cB] = to_num(dfp[cB])
        dfp = dfp.dropna(subset=[cA, cB])
        if dfp.empty:
            ax.axis("off")
            continue
        for _, row in dfp.iterrows():
            a = str(row["agent"]) 
            y0 = float(row[cA])
            y1 = float(row[cB])
            st = agent_style(a)
            ax.plot([0, 1], [y0, y1], color=st["color"], lw=1.4, alpha=0.95, ls=st["linestyle"])
            ax.scatter([0, 1], [y0, y1], color=st["color"], s=10, zorder=3, marker=st["marker"])
        # Add median dashed black line
        medA = float(np.nanmedian(dfp[cA].to_numpy()))
        medB = float(np.nanmedian(dfp[cB].to_numpy()))
        ax.plot([0, 1], [medA, medB], color="black", lw=3.0, ls="--")
        ax.set_xticks([0, 1], [labelA, labelB])
        ax.set_title(title)
        if "CBN-parameter" in title:
            ax.set_ylim(0.0, 1.0)
        # Remove y-labels per request; titles already describe the panel
        ax.set_ylabel("")
        ax.grid(True, axis="y", alpha=0.25)
        used_positions.add((r, c))
    # no-op: legend is figure-level below; nothing to store per-axis

    for rr in range(nrows):
        for cc in range(ncols):
            if (rr, cc) not in used_positions:
                axes_arr[rr, cc].axis("off")

    # Single, figure-level legend at the bottom-right (see layout knobs below).
    from matplotlib.lines import Line2D as _L2D
    median_handle = _L2D([0], [0], color="black", lw=2.5, ls="--", label="Median")
    agent_handles: List[_L2D] = []
    for a in agents:
        st = agent_style(a)
        agent_handles.append(_L2D([0], [0], color=st["color"], lw=2.0, ls=st["linestyle"], marker=st["marker"], label=str(a)))
    handles = [median_handle] + agent_handles
    # Use a figure-level legend so it cannot overlap a panel or be clipped.
    # fig.legend(
    #     handles=handles,
    #     loc="lower center",
    #     bbox_to_anchor=(0.5, 0.02),  # (x, y) in figure coords; raise y to move legend upward
    #     ncol=4,
    #     frameon=True,
    #     title="Agents",
    #     borderaxespad=0.5,
    #     columnspacing=0.7,
    #     handletextpad=0.6,
    # )

    fig.legend(
        handles=handles,
        loc="lower center",
        bbox_to_anchor=(0.5, 0.02),  # (x, y) in figure coords; raise y to move legend upward
        ncol=4,
        frameon=True,
        title="Agents",
        borderaxespad=0.5,
        columnspacing=0.7,
        handletextpad=0.6,
    )

    # Figure title with pretty experiment names if provided in W
    try:
        pc_val = str(W.get("prompt_category", pd.Series([None])).iloc[0] or "")
    except Exception:
        pc_val = ""
    pc_pretty = PC_PRINT_MAP.get(pc_val.lower(), pc_val)
    # Push the suptitle up to reclaim top whitespace.
    fig.suptitle(f"{labelA} → {labelB} ({pc_pretty}); N={int(W['agent'].nunique())} matched agents", y=0.985)
    fig.suptitle(f"{labelA} → {labelB}; N={int(W['agent'].nunique())} matched agents", y=0.985)

    out_path.parent.mkdir(parents=True, exist_ok=True)
    # Reserve more bottom margin for the legend and increase spacing between panels.
    fig.tight_layout(rect=(0.04, 0.16, 0.98, 0.96))
    fig.subplots_adjust(wspace=0.55, hspace=0.2)

    fig.savefig(out_path, bbox_inches="tight", pad_inches=0.03)
    plt.close(fig)

########################################################
########################################################
########################################################

def report_topk_param_differences_pcnum_to_pccot(W: pd.DataFrame, out_dir: Path, k: int = 10) -> Path:
    """Report top-k most/least absolute parameter differences (CoT − Numeric) per experiment.

    Saves a single CSV with rows: experiment, agent, param, pcnum, pccot, delta, abs_delta,
    rank_type in {most, least}, rank (1..k).
    """
    out_dir.mkdir(parents=True, exist_ok=True)
    if W.empty:
        return out_dir / "topk_param_diffs_pcnum_to_pccot.csv"

    rows = []
    exp = str(W.get("experiment", pd.Series([None])).iloc[0]) if len(W) else None

    def collect(param: str, s_num: pd.Series, s_cot: pd.Series) -> None:
        X = pd.DataFrame({
            "agent": W["agent"].astype(str),
            "pcnum": pd.to_numeric(s_num, errors="coerce"),
            "pccot": pd.to_numeric(s_cot, errors="coerce"),
        }).dropna(subset=["pcnum", "pccot"]).copy()
        if X.empty:
            return
        X["delta"] = X["pccot"] - X["pcnum"]
        X["abs_delta"] = X["delta"].abs()
        X["param"] = param

        # top-k most changed
        most = X.sort_values("abs_delta", ascending=False).head(k).copy()
        most["rank_type"], most["rank"] = "most", range(1, len(most) + 1)
        # top-k least changed (ties include zeros)
        least = X.sort_values(["abs_delta", "agent"], ascending=[True, True]).head(k).copy()
        least["rank_type"], least["rank"] = "least", range(1, len(least) + 1)

        for df in (most, least):
            for _, r in df.iterrows():
                rows.append({
                    "experiment": exp,
                    "agent": r["agent"],
                    "param": param,
                    "pcnum": float(r["pcnum"]),
                    "pccot": float(r["pccot"]),
                    "delta": float(r["delta"]),
                    "abs_delta": float(r["abs_delta"]),
                    "rank_type": r["rank_type"],
                    "rank": int(r["rank"]),
                })

    # standard params
    for p in ["b", "m1", "m2"]:
        c_num, c_cot = f"{p}__pcnum", f"{p}__pccot"
        if c_num in W.columns and c_cot in W.columns:
            collect(p, W[c_num], W[c_cot])

    # averaged pC across two causes
    if {"pC1__pcnum", "pC2__pcnum"}.issubset(W.columns) and {"pC1__pccot", "pC2__pccot"}.issubset(W.columns):
        pc_num = (pd.to_numeric(W["pC1__pcnum"], errors="coerce") + pd.to_numeric(W["pC2__pcnum"], errors="coerce")) / 2.0
        pc_cot = (pd.to_numeric(W["pC1__pccot"], errors="coerce") + pd.to_numeric(W["pC2__pccot"], errors="coerce")) / 2.0
        collect("pCavg", pc_num, pc_cot)

    T = pd.DataFrame(rows)
    out_path = out_dir / "topk_param_diffs_pcnum_to_pccot.csv"
    T.to_csv(out_path, index=False)
    return out_path


# ---- Top-k agents export from experiment-pair master ----
def plot_box_params_by_experiment_and_pc(master: pd.DataFrame, out_dest: Path) -> None:
    """2×2 grouped boxplots by experiment×prompt-category for b, m1, m2, pCavg.

    Saves figure as box_params_by_experiment_and_pc.pdf in the provided directory (or
    at out_dest if it's a full file path). Also writes CSV summaries and outliers:
    - box_params_by_experiment_and_pc_summary.csv
    - box_params_by_experiment_and_pc_outliers.csv
    """
    df = master.copy()
    df = df[df["prompt_category"].isin(["pcnum", "pccot"])].copy()
    if df.empty:
        return

    # Prepare pCavg per row (if pC1/pC2 exist)
    if {"pC1", "pC2"}.issubset(df.columns):
        df["pCavg"] = (to_num(df["pC1"]) + to_num(df["pC2"])) / 2.0
    else:
        df["pCavg"] = np.nan

    params = ["b", "m1", "m2", "pCavg"]
    exps = sorted(df["experiment"].unique().tolist(), reverse=True)

    # Normalize output path
    out_dest = Path(out_dest)
    if out_dest.suffix.lower() == ".pdf":
        out_path = out_dest
        out_dir = out_dest.parent
    else:
        out_dir = out_dest
        out_dir.mkdir(parents=True, exist_ok=True)
        out_path = out_dir / "box_params_by_experiment_and_pc.pdf"
    out_dir.mkdir(parents=True, exist_ok=True)

    fig, axes = plt.subplots(2, 2, figsize=(11.5, 7.2), sharex=True)
    rows = []
    outlier_rows = []
    for idx, p in enumerate(params):
        ax = axes[idx // 2, idx % 2]
        num_data, cot_data = [], []
        for e in exps:
            sub = df[df["experiment"] == e]
            num_vals = to_num(sub[sub["prompt_category"] == "pcnum"][p]).dropna().values
            cot_vals = to_num(sub[sub["prompt_category"] == "pccot"][p]).dropna().values
            num_data.append(num_vals)
            cot_data.append(cot_vals)

        group_spacing = 1.20
        base = np.arange(len(exps)) * group_spacing
        pair_offset = 0.2
        positions_num = base - pair_offset
        positions_cot = base + pair_offset

        bp1 = ax.boxplot(
            num_data,
        
            positions=positions_num,
            widths=0.26,
            patch_artist=True,
            showmeans=True,
            meanprops=dict(marker='^', markerfacecolor='yellow')
        )
        bp2 = ax.boxplot(
            cot_data,
            positions=positions_cot,
            widths=0.26,
            patch_artist=True,
            showmeans=True,
            meanprops=dict(marker='^', markerfacecolor='yellow')
        )
        for patch in bp1['boxes']:
            patch.set_facecolor(numeric_color)
            patch.set_edgecolor('black')
        for patch in bp2['boxes']:
            patch.set_facecolor(cot_color)
            patch.set_edgecolor('black')
        for median in bp1['medians']:
            median.set_color('black')
        for median in bp2['medians']:
            median.set_color('black')

        ax.set_xticks(base)
        if idx // 2 == 1:
            ax.set_xticklabels([str(exp_name_map.get(e, e) or "") for e in exps], rotation=25)
            ax.set_xlabel('Experiment')
        else:
            ax.set_xticklabels([])
        ax.set_ylabel(p if p != "pCavg" else "p(C)")
        ax.set_ylim(0.0, 1.0)
        ax.grid(axis='y', linestyle=':', alpha=0.4)
        ax.set_title(f"CBN-parameter {p if p != 'pCavg' else 'p(C)'}")

        # CSV rows per experiment x pc for this parameter (summary + outliers)
        for i, e in enumerate(exps):
            for pc, vals in ("pcnum", num_data[i]), ("pccot", cot_data[i]):
                s = pd.to_numeric(pd.Series(vals), errors="coerce").dropna()
                summ = _iqr_summary(s)
                rows.append({
                    "experiment": e,
                    "prompt_category": pc,
                    "param": p,
                    **summ,
                })
                lf, uf = summ["lower_fence"], summ["upper_fence"]
                sub = df[(df["experiment"] == e) & (df["prompt_category"] == pc)][["agent", p]].copy()
                sub[p] = to_num(sub[p])
                mask = (sub[p] < lf) | (sub[p] > uf)
                for _, r in sub[mask].iterrows():
                    outlier_rows.append({
                        "experiment": e,
                        "prompt_category": pc,
                        "param": p,
                        "agent": r["agent"],
                        "value": float(r[p]),
                        "lower_fence": lf,
                        "upper_fence": uf,
                    })

    handles = [Patch(facecolor=numeric_color), Patch(facecolor=cot_color),
               Line2D([], [], color='yellow', marker='^', linestyle='None'),
               Line2D([], [], color='black', linestyle='-')]
    labels = ['Numeric', 'CoT', 'Mean', 'Median']
    fig.legend(handles, labels, loc='upper center', ncol=4, frameon=True)
    fig.tight_layout(rect=(0, 0, 1, 0.96))
    # Write CSVs next to the figure
    pd.DataFrame(rows).to_csv(out_dir / "box_params_by_experiment_and_pc_summary.csv", index=False)
    pd.DataFrame(outlier_rows).to_csv(out_dir / "box_params_by_experiment_and_pc_outliers.csv", index=False)

    fig.savefig(out_path, dpi=200)
    plt.close(fig)


def plot_param_delta_violin_exp_pair(
    W: pd.DataFrame,
    suffixA: str,
    suffixB: str,
    out_path: Path,
) -> None:
    """Violin + swarm for deltas (B − A) across parameters for experiment pairs within one prompt-category."""
    if W.empty:
        return

    # Pretty experiment names for labels and count of matched agents
    try:
        expA_raw = str(W.get("expA", pd.Series([None])).iloc[0])
        expB_raw = str(W.get("expB", pd.Series([None])).iloc[0])
    except Exception:
        expA_raw, expB_raw = "A", "B"
    labelA = str(exp_name_map.get(expA_raw, expA_raw) or expA_raw)
    labelB = str(exp_name_map.get(expB_raw, expB_raw) or expB_raw)
    n_agents = int(W["agent"].nunique())

    deltas: Dict[str, np.ndarray] = {}
    for p in ["b", "m1", "m2"]:
        cA, cB = f"{p}{suffixA}", f"{p}{suffixB}"
        if cA in W.columns and cB in W.columns:
            a = pd.to_numeric(W[cA], errors="coerce")
            b = pd.to_numeric(W[cB], errors="coerce")
            deltas[p] = (b - a).to_numpy()
    if {f"pC1{suffixA}", f"pC2{suffixA}"}.issubset(W.columns) and {f"pC1{suffixB}", f"pC2{suffixB}"}.issubset(W.columns):
        pA = (pd.to_numeric(W[f"pC1{suffixA}"], errors="coerce") + pd.to_numeric(W[f"pC2{suffixA}"], errors="coerce")) / 2.0
        pB = (pd.to_numeric(W[f"pC1{suffixB}"], errors="coerce") + pd.to_numeric(W[f"pC2{suffixB}"], errors="coerce")) / 2.0
        deltas["pCavg"] = (pB - pA).to_numpy()

    if not deltas:
        return

    params = [p for p in ["b", "m1", "m2", "pCavg"] if p in deltas]
    nP = len(params)
    nrows, ncols = (2, 2) if nP > 2 else (1, nP)
    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(8.8, 5.6))
    if not isinstance(axes, np.ndarray):
        axes = np.array([axes])
    axes = axes.reshape(nrows, ncols)

    all_vals = np.concatenate([np.asarray(v, dtype=float) for v in deltas.values()])
    all_vals = all_vals[np.isfinite(all_vals)]
    if all_vals.size == 0:
        return
    vmax = float(np.nanmax(np.abs(all_vals)))
    ylim = (-max(0.1, vmax * 1.1), max(0.1, vmax * 1.1))

    i = -1
    for i, p in enumerate(params):
        r, c = divmod(i, ncols)
        ax = axes[r, c]
        x = np.asarray(deltas[p], dtype=float)
        x = x[np.isfinite(x)]
        if x.size == 0:
            ax.axis("off")
            continue
        parts = ax.violinplot([x], positions=[0], showextrema=False)
        for pc in parts['bodies']:
            pc.set_facecolor("#ddeaff")
            pc.set_edgecolor("#6a89cc")
            pc.set_alpha(0.9)
        rng = np.random.default_rng(404 + i)
        jitter = rng.uniform(-0.06, 0.06, size=x.size)
        ax.scatter(jitter, x, s=14, color="gray", alpha=0.6, edgecolors="none", zorder=3)
        ax.axhline(0.0, color="black", lw=1.0, alpha=0.6)
        med = float(np.nanmedian(x))
        lo, hi = _bootstrap_ci(x, q=0.5)
        ax.scatter([0], [med], color="red", s=36, zorder=4, label="Median")
        if np.isfinite(lo) and np.isfinite(hi):
            ax.plot([0, 0], [lo, hi], color="red", lw=2.2, alpha=0.9)
        ax.set_xlim(-0.25, 0.25)
        ax.set_ylim(*ylim)
        ax.set_xticks([0])
        ax.set_xticklabels([_pretty_param_label(p)])
        if c == 0:
            ax.set_ylabel(f"Delta ({labelB} - {labelA})")

    for j in range((i + 1) if i >= 0 else 0, nrows * ncols):
        r, c = divmod(j, ncols)
        axes[r, c].axis("off")

    # Compose a clearer title with pretty experiment names and N common agents in prompt category
    try:
        pc_val = str(W.get("prompt_category", pd.Series([None])).iloc[0] or "")
    except Exception:
        pc_val = ""
    _pc_pretty = {"pcnum": "Numeric", "pccot": "CoT", "pcconf": "Numeric-Conf"}
    pc_pretty = _pc_pretty.get(pc_val.lower(), pc_val)

    fig.suptitle(f"{labelB} vs {labelA}: N={n_agents} common agents in {pc_pretty}")

    # Add a clear figure-level legend for markers
    legend_handles = [
        Line2D([], [], color="red", marker="o", lw=2.0, label="Median and CI"),
        Line2D([], [], color="gray", marker="o", linestyle="None", label="Agent deltas"),
    ]
    fig.legend(handles=legend_handles, loc="upper right", frameon=True)

    # CSV exports for summary, per-agent and outliers
    out_dir = out_path.parent
    out_dir.mkdir(parents=True, exist_ok=True)
    sum_rows = []
    outlier_rows = []
    per_agent_rows = []
    for p in params:
        if p == "pCavg":
            pA = (pd.to_numeric(W[f"pC1{suffixA}"], errors="coerce") + pd.to_numeric(W[f"pC2{suffixA}"], errors="coerce")) / 2.0
            pB = (pd.to_numeric(W[f"pC1{suffixB}"], errors="coerce") + pd.to_numeric(W[f"pC2{suffixB}"], errors="coerce")) / 2.0
            d = (pB - pA)
        else:
            d = pd.to_numeric(W[f"{p}{suffixB}"], errors="coerce") - pd.to_numeric(W[f"{p}{suffixA}"], errors="coerce")
        summ = _iqr_summary(d)
        sum_rows.append({"param": p, **summ})
        tmp = pd.DataFrame({"agent": W["agent"], "delta": d})
        per_agent_rows.extend([{ "param": p, "agent": a, "delta": float(v)} for a, v in tmp.dropna().to_records(index=False)])
        lf, uf = summ["lower_fence"], summ["upper_fence"]
        for _, r in tmp.dropna().iterrows():
            if r["delta"] < lf or r["delta"] > uf:
                outlier_rows.append({"param": p, "agent": r["agent"], "delta": float(r["delta"]),
                                     "lower_fence": lf, "upper_fence": uf})

    pd.DataFrame(sum_rows).to_csv(out_dir / "delta_violin_exp_pair_summary.csv", index=False)
    pd.DataFrame(per_agent_rows).to_csv(out_dir / "delta_violin_exp_pair_per_agent.csv", index=False)
    pd.DataFrame(outlier_rows).to_csv(out_dir / "delta_violin_exp_pair_outliers.csv", index=False)

    fig.tight_layout(rect=(0, 0, 1, 0.94))
    fig.savefig(out_path)
    plt.close(fig)


def plot_param_delta_violin_pcnum_to_pccot(W: pd.DataFrame, out_path: Path) -> None:
    """Raincloud/violin + swarm for deltas (pccot - pcnum) across parameters.

    Produces a 2x2 panel (b, m1, m2, pCavg). Includes zero reference line, median
    marker per parameter, and a thin CI bar via bootstrap.
    """
    if W.empty:
        return

    # Assemble deltas
    deltas = {}
    for p in ["b", "m1", "m2"]:
        cA, cB = f"{p}__pcnum", f"{p}__pccot"
        if cA in W.columns and cB in W.columns:
            a = pd.to_numeric(W[cA], errors="coerce")
            b = pd.to_numeric(W[cB], errors="coerce")
            deltas[p] = (b - a).to_numpy()
    if {"pC1__pcnum", "pC2__pcnum"}.issubset(W.columns) and {"pC1__pccot", "pC2__pccot"}.issubset(W.columns):
        pA = (pd.to_numeric(W["pC1__pcnum"], errors="coerce") + pd.to_numeric(W["pC2__pcnum"], errors="coerce")) / 2.0
        pB = (pd.to_numeric(W["pC1__pccot"], errors="coerce") + pd.to_numeric(W["pC2__pccot"], errors="coerce")) / 2.0
        deltas["pCavg"] = (pB - pA).to_numpy()

    if not deltas:
        return

    # Figure layout
    params = ["b", "m1", "m2", "pCavg"]
    params = [p for p in params if p in deltas]
    nP = len(params)
    nrows, ncols = (2, 2) if nP > 2 else (1, nP)
    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(8.8, 5.6))
    if not isinstance(axes, np.ndarray):
        axes = np.array([axes])
    axes = axes.reshape(nrows, ncols)

    # y-range based on data; keep symmetric
    all_vals = np.concatenate([np.asarray(v, dtype=float) for v in deltas.values()])
    all_vals = all_vals[np.isfinite(all_vals)]
    if all_vals.size == 0:
        return
    vmax = float(np.nanmax(np.abs(all_vals)))
    ylim = (-max(0.1, vmax * 1.1), max(0.1, vmax * 1.1))

    i = -1
    for i, p in enumerate(params):
        r, c = divmod(i, ncols)
        ax = axes[r, c]
        x = np.asarray(deltas[p], dtype=float)
        x = x[np.isfinite(x)]
        if x.size == 0:
            ax.axis("off")
            continue
        # violin
        parts = ax.violinplot([x], positions=[0], showextrema=False)
        for pc in parts['bodies']:
            pc.set_facecolor("#ddeaff")
            pc.set_edgecolor("#6a89cc")
            pc.set_alpha(0.9)
        # swarm (jittered)
        rng = np.random.default_rng(2025 + i)
        jitter = rng.uniform(-0.06, 0.06, size=x.size)
        ax.scatter(jitter, x, s=14, color="gray", alpha=0.6, edgecolors="none", zorder=3)
        # zero line
        ax.axhline(0.0, color="black", lw=1.0, alpha=0.6)
        # median and CI
        med = float(np.nanmedian(x))
        lo, hi = _bootstrap_ci(x, q=0.5)
        ax.scatter([0], [med], color="red", s=36, zorder=4, label="Median")
        if np.isfinite(lo) and np.isfinite(hi):
            ax.plot([0, 0], [lo, hi], color="red", lw=2.2, alpha=0.9)
        # cosmetics
        ax.set_xlim(-0.25, 0.25)
        ax.set_ylim(*ylim)
        ax.set_xticks([0])
        ax.set_xticklabels([_pretty_param_label(p)])
        if c == 0:
            ax.set_ylabel("Delta (CoT - Numeric)")

    # Hide unused subplots
    for j in range((i + 1) if i >= 0 else 0, nrows * ncols):
        r, c = divmod(j, ncols)
        axes[r, c].axis("off")

    # Title: pretty experiment name + N common agents
    try:
        exp_raw = str(W.get("experiment", pd.Series([None])).iloc[0])
    except Exception:
        exp_raw = ""
    exp_pretty = str(exp_name_map.get(exp_raw, exp_raw) or exp_raw)
    n_agents = int(W["agent"].nunique())

    fig.suptitle(f"{exp_pretty}: N={n_agents} common agents in CoT and Numeric")

    # Add a clear figure-level legend for markers
    legend_handles = [
        Line2D([], [], color="red", marker="o", lw=2.0, label="Median and CI"),
        Line2D([], [], color="gray", marker="o", linestyle="None", label="Agent deltas"),
    ]
    fig.legend(handles=legend_handles, loc="upper right", frameon=True)

    # CSV exports: per-parameter summaries and per-agent deltas with outliers
    out_dir = out_path.parent
    out_dir.mkdir(parents=True, exist_ok=True)
    # per-parameter summary
    sum_rows = []
    outlier_rows = []
    per_agent_rows = []
    # Build agent-level deltas, too
    for p in params:
        if p == "pCavg":
            pA = (pd.to_numeric(W["pC1__pcnum"], errors="coerce") + pd.to_numeric(W["pC2__pcnum"], errors="coerce")) / 2.0
            pB = (pd.to_numeric(W["pC1__pccot"], errors="coerce") + pd.to_numeric(W["pC2__pccot"], errors="coerce")) / 2.0
            d = (pB - pA)
        else:
            d = pd.to_numeric(W[f"{p}__pccot"], errors="coerce") - pd.to_numeric(W[f"{p}__pcnum"], errors="coerce")
        # per-parameter summary
        summ = _iqr_summary(d)
        sum_rows.append({"param": p, **summ})
        # agent-level deltas and outliers
        tmp = pd.DataFrame({"agent": W["agent"], "delta": d})
        per_agent_rows.extend([{"param": p, "agent": a, "delta": float(v)} for a, v in tmp.dropna().to_records(index=False)])
        lf, uf = summ["lower_fence"], summ["upper_fence"]
        for _, r in tmp.dropna().iterrows():
            if r["delta"] < lf or r["delta"] > uf:
                outlier_rows.append({"param": p, "agent": r["agent"], "delta": float(r["delta"]),
                                     "lower_fence": lf, "upper_fence": uf})

    pd.DataFrame(sum_rows).to_csv(out_dir / "delta_violin_pcnum_to_pccot_summary.csv", index=False)
    pd.DataFrame(per_agent_rows).to_csv(out_dir / "delta_violin_pcnum_to_pccot_per_agent.csv", index=False)
    pd.DataFrame(outlier_rows).to_csv(out_dir / "delta_violin_pcnum_to_pccot_outliers.csv", index=False)

    fig.tight_layout(rect=(0, 0, 1, 0.94))
    fig.savefig(out_path)
    plt.close(fig)


def plot_slope_params_pcnum_to_pccot(W: pd.DataFrame, out_dir: Path,
                                      highlight_max_legend_agents: int = 12) -> None:
    """Paired slope plots for parameters (pcnum → pccot) per agent.

    Parameters
    ----------
    W : DataFrame
        Output of matched_pc(...), with columns like b__pcnum, b__pccot, etc.
    out_dir : Path
        Directory to write one PDF per parameter.
    """
    out_dir.mkdir(parents=True, exist_ok=True)
    if W.empty:
        return

    agents = W["agent"].astype(str)

    def _plot_pair(valsA: pd.Series, valsB: pd.Series, param: str):
        X = pd.DataFrame({"agent": agents, "A": pd.to_numeric(valsA, errors="coerce"),
                          "B": pd.to_numeric(valsB, errors="coerce")}).dropna(subset=["A", "B"])
        if X.empty:
            return
        # highlight sets from baseline A
        base_df = X[["agent", "A"]].rename(columns={"A": "loocv_r2"})
        _, _, legend_bottom, legend_top = _top_bottom_agents_for_baseline(base_df, highlight_max_legend_agents)
        special = legend_bottom + legend_top
        cmap = _agent_color_map(special)

        fig, ax = plt.subplots(figsize=(7.2, 4.2))
        xA, xB = 0, 1
        for _, r in X.iterrows():
            a = str(r["agent"])  # baseline and comparison
            yA = float(r["A"])
            yB = float(r["B"])
            color = cmap.get(a, (0.7, 0.7, 0.7))
            lw = 1.8 if a in cmap else 1.0
            alpha = 0.9 if a in cmap else 0.6
            ax.plot([xA, xB], [yA, yB], color=color, lw=lw, alpha=alpha)
            if a in cmap:
                ax.scatter([xA, xB], [yA, yB], s=26, color=color, zorder=3)

        medA = float(np.nanmedian(X["A"].to_numpy()))
        medB = float(np.nanmedian(X["B"].to_numpy()))
        ax.plot([xA, xB], [medA, medB], color="red", lw=3.0, alpha=0.9, label="Median change")

        ax.set_xticks([xA, xB], ["Numeric", "CoT"])
        ax.set_ylabel(_pretty_param_label(param))
        # parameters are probabilities per user note
        ax.set_ylim(0.0, 1.0)
        n = X["agent"].nunique()
        ax.set_title(f"{_pretty_param_label(param)}: pcnum → pccot; N={n} agents")

        handles = [Line2D([0], [0], color="red", lw=3.0, label="Median change")]
        for a in special:
            handles.append(Line2D([0], [0], color=cmap[a], lw=2.0, label=a))
        if len(handles) > 1:
            ax.legend(handles=handles, loc="best", frameon=False, title="Highlighted", ncol=2)

        ax.grid(True, axis="y", alpha=0.2)
        fig.tight_layout()
        out_path = out_dir / f"slope_{param}_pcnum_to_pccot.pdf"
        fig.savefig(out_path)
        plt.close(fig)

    # Plot for b, m1, m2 if present
    for p in ["b", "m1", "m2"]:
        cA, cB = f"{p}__pcnum", f"{p}__pccot"
        if cA in W.columns and cB in W.columns:
            _plot_pair(W[cA], W[cB], p)

    # pCavg = mean of pC1 and pC2
    # identify pC availability
    if {"pC1__pcnum", "pC2__pcnum"}.issubset(W.columns) and {"pC1__pccot", "pC2__pccot"}.issubset(W.columns):
        pC_A = (pd.to_numeric(W["pC1__pcnum"], errors="coerce") + pd.to_numeric(W["pC2__pcnum"], errors="coerce")) / 2.0
        pC_B = (pd.to_numeric(W["pC1__pccot"], errors="coerce") + pd.to_numeric(W["pC2__pccot"], errors="coerce")) / 2.0
        _plot_pair(pC_A, pC_B, "pCavg")


def tex_per_condition_medians(master: pd.DataFrame, out_path: Path) -> None:
    """Write a LaTeX table for per-condition medians and 3-parameter share.

    Columns include average tokens, median R^2, b, m1, m2, median p(C), and the
    share of agents selecting a 3-parameter model ("num/den"). Highlights the most
    normative values per experiment and overall.

    What this reveals
    -----------------
    Compact, publication-ready summary of central tendencies across all conditions,
    suitable for manuscript tables (complements per_condition_medians.csv).
    """
    df = master.copy()
    def med(s): return float(np.median(to_num(s).dropna())) if s is not None else np.nan

    # # --- mappings ---
    # exp_name_map = {
    #     "random_abstract": "Abstract",
    #     "rw17_indep_causes": "RW17",
    #     "abstract_overloaded_lorem_de": "Abstract-Over-DE",
    #     "rw17_overloaded_de": "RW17-Over-DE",
    #     "rw17_overloaded_d": "RW17-Over-D",
    #     "rw17_overloaded_e": "RW17-Over-E",
    # }
    pc_print_map = {"pccot": "CoT", "pcnum": "Numeric", "pcconf": "Numeric-Conf"}

    # --- rows ---
    rows = []
    for (exp, pc), sub in df.groupby(["experiment", "prompt_category"], dropna=False):
        rows.append({
            "experiment": exp,
            "prompt_category": pc,
            "Tokens": avg_tokens_for(exp, pc),  # floored in avg_tokens_for
            "R2": med(sub["loocv_r2"]),
            "b": med(sub["b"]),
            "m1": med(sub["m1"]),
            "m2": med(sub["m2"]),
            "pC": med((to_num(sub["pC1"]) + to_num(sub["pC2"])) / 2.0),
            "three_par": share_three_param(sub["params_tying"]),  # e.g., "25/30"
        })

    T = pd.DataFrame(rows)

    # --- global winners/losers (bold) ---
    def _is_global_max(col):
        mx = T[col].max(skipna=True)
        return T[col].notna() & np.isclose(T[col], mx)

    def _is_global_min(col):
        mn = T[col].min(skipna=True)
        return T[col].notna() & np.isclose(T[col], mn)

    T["Tokens_is_gmax"] = _is_global_max("Tokens")
    T["R2_is_gmax"]     = _is_global_max("R2")
    T["m1_is_gmax"]     = _is_global_max("m1")
    T["m2_is_gmax"]     = _is_global_max("m2")
    T["b_is_gmin"]      = _is_global_min("b")


    # --- per-experiment higher (italics) ---
    def _is_local_high(col):
        g = T.groupby("experiment")[col]
        gmax = g.transform("max")
        gmin = g.transform("min")
        ties = np.isclose(gmax, gmin)  # if both rows equal, no italics
        return T[col].notna() & np.isclose(T[col], gmax) & (~ties)

    def _is_local_min(col):
        g = T.groupby("experiment")[col]
        gmax = g.transform("max")
        gmin = g.transform("min")
        ties = np.isclose(gmax, gmin)  # if both rows equal, no italics
        return T[col].notna() & np.isclose(T[col], gmin) & (~ties)

    # for c in ["Tokens", "R2", "b", "m1", "m2"]:
    for c in [ "R2",  "m1", "m2"]:

        T[c + "_is_local_high"] = _is_local_high(c)
    for c in ["b"]:
        T[c + "_is_local_min"] = _is_local_min(c)

    # --- print labels & order ---
    T["exp_print"] = T["experiment"].map(exp_name_map).fillna(T["experiment"])
    T["prompt_print"] = T["prompt_category"].map(pc_print_map).fillna(T["prompt_category"])
    T["prompt_order"] = T["prompt_print"].map({"Numeric": 0, "CoT": 1}).fillna(9)
    T = T.sort_values(["exp_print", "prompt_order"])

    # --- helpers for styling ---
    def style_num(val, bold=False, ital=False, decimals=3, as_int=False):
        if pd.isna(val):
            return ""
        s = f"{int(val):d}" if as_int else f"{val:.{decimals}f}"
        if ital:
            s = f"\\underline{{{s}}}"
        if bold:
            s = f"\\textbf{{{s}}}"
        return s

    lines = []
    lines.append("% Auto-generated; do not edit by hand\n")
    lines.append("\\begin{table}[t]\n")
    lines.append("\\centering\n")
    lines.append(
        "\\caption{Per-condition medians, average tokens, and 3-par share "
        "denoting the number of agents whose selected model uses 3 parameters, "
        "over the total number of agents in that condition. "
        "Only RW17 Numeric includes human data (counted as one agent)."
        "For CBN-parameters and $R^2$, we report most normative median values per experiment \\underline{underlined} and overall \\textbf{bolded}."
        "}\n"
    )
    lines.append("\\label{tab:cbn_per_condition_medians}\n")
    # 9 columns: Experiment, Prompt, Tokens, R^2, b, m1, m2, p(C), 3-par
    lines.append("\\begin{tabular}{l l r r r r r r r}\n")
    lines.append("\\toprule\n")
    lines.append("Experiment & Prompt & Tokens & $R^2$ & $b$ & $m_1$ & $m_2$ & $p(C)$ & 3-par \\\\\n")
    lines.append("\\midrule\n")

    # for _, r in T.iterrows():
    for i,(idx, r) in enumerate(T.iterrows()):
        # tok = style_num(
        #     r["Tokens"],
        #     bold=r["Tokens_is_gmax"],
        #     ital=r["Tokens_is_local_high"],
        #     as_int=True,
        # )
        r2 = style_num(
            r["R2"],
            bold=r["R2_is_gmax"],
            ital=r["R2_is_local_high"],
        )
        b_  = style_num(
            r["b"],
            bold=r["b_is_gmin"],
            ital=r["b_is_local_min"],
        )
        m1 = style_num(
            r["m1"],
            bold=r["m1_is_gmax"],
            ital=r["m1_is_local_high"],
        )
        m2 = style_num(
            r["m2"],
            bold=r["m2_is_gmax"],
            ital=r["m2_is_local_high"],
        )

        lines.append(
            f"{latex_escape(r['exp_print'])} & {latex_escape(r['prompt_print'])} & "
            # f"{tok} & {r2} & {b_} & {m1} & {m2} & "
            f"{r['Tokens']} &  {r2} & {b_} & {m1} & {m2} & "
            f"{r['pC']:.3f} & {r['three_par']} \\\\\n"
        )

        if (i + 1) % 2 == 0 and (i + 1) != len(T):
            lines.append("\\arrayrulecolor[gray]{0.8}\\hline\\arrayrulecolor{black}\n")

    lines.append("\\bottomrule\n")
    lines.append("\\end{tabular}\n")
    lines.append("\\end{table}\n")
    out_path.write_text("".join(lines))




def tex_paired_summary(T: pd.DataFrame, caption: str, label: str, out_path: Path) -> None:
    """Write a LaTeX table for paired test summaries (BH–FDR adjusted).

    The input table T should contain columns: metric, N, median_delta, wilcoxon_stat,
    p_value, p_fdr. This renders a clean table with median deltas and adjusted p-values.
    Complements paired_summary.csv in the corresponding comparison folder.
    """
    # Keep core stats, format cleanly
    disp = T.copy()
    # Order by metric for stable layout
    disp = disp.sort_values("metric")
    lines = []
    lines.append("% Auto-generated; do not edit by hand\n")
    lines.append("\\begin{table}[t]\n\\centering\n")
    lines.append(f"\\caption{{{caption}}}\n")
    lines.append(f"\\label{{{label}}}\n")
    lines.append("\\begin{tabular}{l r r r r}\n\\toprule\n")
    lines.append("Metric & $N$ & Median $\\Delta$ & Wilcoxon & $p_\\mathrm{FDR}$ \\\\\n\\midrule\n")
    for _, r in disp.iterrows():
        med = r["median_delta"]
        w = r["wilcoxon_stat"]
        pfdr = r["p_fdr"]
        lines.append(f"{latex_escape(r['metric'])} & {int(r['N'])} & {med if pd.isna(med) else f'{med:.3f}'} & "
                     f"{w if pd.isna(w) else f'{w:.3f}'} & {pfdr if pd.isna(pfdr) else f'{pfdr:.3g}'} \\\\\n")
    lines.append("\\bottomrule\n\\end{tabular}\n\\end{table}\n")
    out_path.write_text("".join(lines))

############################################################################
############################################################################
# ---- Top-k agents export from experiment-pair master ----
def export_topk_agents_from_exp_pair_master(master_csv, k: int = 3):
    """Extract top-k most/least changing agents per condition×parameter from the
    aggregated experiment-pair master CSV and export both CSV and LaTeX tables.

    Inputs
    - master_csv: Path to results/parameter_analysis/cbn_agg/topk_master_experiment_pairs.csv
    - k: number of agents to include in most/least lists (default 3)

    Behavior
    - Groups by (exp_pair_prompt_category, comparison, sideA, sideB, prompt_category, param).
    - For each group, collects up to k rows with smallest rank for rank_type == 'most'
      and similarly for rank_type == 'least'. Falls back to sorting by abs_delta if
      'rank' is missing.
    - Writes a long-form CSV with one row per (condition×param×rank_type×rank):
      columns [exp_pair_prompt_category, comparison, sideA, sideB, prompt_category,
      param, rank_type, rank, agent, delta, abs_delta].
    - Writes a compact LaTeX table with one row per (condition×param), and two columns
      that list the top-k most/least agents with their |Δ| values.

    Outputs
    - Returns (csv_path, tex_path). Files are placed in <master_csv_dir>/top_k/ as
      top_{k}_agents.csv and top_{k}_agents.tex.
    """
    mpath = Path(master_csv)
    if not mpath.exists():
        raise FileNotFoundError(f"Master CSV not found: {mpath}")

    out_dir = mpath.parent / "top_k"
    out_dir.mkdir(parents=True, exist_ok=True)

    df = pd.read_csv(mpath)

    # Normalize columns we rely on
    if "abs_delta" not in df.columns and "delta" in df.columns:
        df["abs_delta"] = pd.to_numeric(df["delta"], errors="coerce").abs()
    # Ensure expected string columns exist
    for c in [
        "exp_pair_prompt_category", "comparison", "sideA", "sideB",
        "prompt_category", "param", "rank_type", "agent"
    ]:
        if c not in df.columns:
            df[c] = None
    # Numeric fields
    for c in ["rank", "abs_delta", "delta"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")

    # Helper: get top-k rows for a given group and rank_type
    def _topk(sub: pd.DataFrame, kind: str) -> pd.DataFrame:
        X = sub[sub["rank_type"] == kind].copy()
        if X.empty:
            return X
        if "rank" in X.columns and X["rank"].notna().any():
            X = X.sort_values(["rank", "abs_delta"], ascending=[True, False])
        else:
            X = X.sort_values(["abs_delta", "agent"], ascending=[False, True]) if kind == "most" \
                else X.sort_values(["abs_delta", "agent"], ascending=[True, True])
        return X.head(k)

    group_cols = [
        "exp_pair_prompt_category", "comparison", "sideA", "sideB", "prompt_category", "param"
    ]

    long_rows: list[dict] = []
    for keys, sub in df.groupby(group_cols, dropna=False):
        sub = sub.copy()
        most = _topk(sub, "most")
        least = _topk(sub, "least")
        for tag, X in [("most", most), ("least", least)]:
            for _, r in X.iterrows():
                long_rows.append({
                    "exp_pair_prompt_category": r.get("exp_pair_prompt_category"),
                    "comparison": r.get("comparison"),
                    "sideA": r.get("sideA"),
                    "sideB": r.get("sideB"),
                    "prompt_category": r.get("prompt_category"),
                    "param": r.get("param"),
                    "rank_type": tag,
                    "rank": int(r.get("rank")) if pd.notna(r.get("rank")) else None,
                    "agent": r.get("agent"),
                    "delta": float(r.get("delta")) if pd.notna(r.get("delta")) else None,
                    "abs_delta": float(r.get("abs_delta")) if pd.notna(r.get("abs_delta")) else None,
                })

    L = pd.DataFrame(long_rows)
    csv_path = out_dir / f"top_{k}_agents.csv"
    L.to_csv(csv_path, index=False)

    # Build a compact LaTeX table
    def _latex_escape(s: str) -> str:
        if s is None:
            return ""
        s = str(s)
        # Minimal escaping for underscores, percent, and ampersand
        return s.replace("\\", "\\textbackslash{}") \
                .replace("_", "\\_") \
                .replace("%", "\\%") \
                .replace("&", "\\&")

    def _pretty_param(p: str) -> str:
        # Local pretty labels to avoid external dependencies
        pretty = {"b": "$b$", "m1": "$m_1$", "m2": "$m_2$", "pCavg": "$p(C)$"}
        return pretty.get(str(p), str(p))

    # Aggregate into wide form strings for LaTeX
    wide_rows: list[dict] = []
    for keys, sub in L.groupby(group_cols, dropna=False):
        most = sub[sub["rank_type"] == "most"].sort_values("rank")
        least = sub[sub["rank_type"] == "least"].sort_values("rank")
        def mk(s: pd.DataFrame) -> str:
            # Report signed delta (B - A) instead of absolute
            parts: list[str] = []
            for row in s.itertuples():
                try:
                    d = float(row.delta)
                except Exception:
                    d = np.nan
                parts.append(f"{row.agent} ({d:+.3f})" if pd.notna(d) else f"{row.agent} (nan)")
            return "; ".join(parts)
        wide_rows.append({
            "exp_pair_prompt_category": keys[0],
            "comparison": keys[1],
            "sideA": keys[2],
            "sideB": keys[3],
            "prompt_category": keys[4],
            "param": keys[5],
            f"top_{k}_most": mk(most),
            f"top_{k}_least": mk(least),
        })

    W = pd.DataFrame(wide_rows)

    # Compose LaTeX with 5 columns: Condition, PC, Param, Most, Least
    # Condition = "{sideA} → {sideB}"; PC printed separately
    lines: list[str] = []
    lines.append("% Auto-generated; do not edit by hand\n")
    lines.append("\\begin{table}[t]\n\\centering\n")
    lines.append("\\scriptsize\n")
    lines.append(f"\\caption{{Top-{k} most/least-changing agents per experiment-pair condition (by signed $\\Delta$; $\\Delta$ = A$-$B) for given prompt category (PC) and parameter (Param).}}\n")
    lines.append("\\label{tab:top_%d_agents_pairs_exp-pairs}\n" % k)
    # Use fixed-width columns for the last two cols to prevent overflow in agent lists
    lines.append("\\begin{tabular}{l l l p{5cm} p{5cm}}\n\\hline\n")
    lines.append("Condition A$-$B & PC & Param & Most & Least \\\ \n\\hline\n")

    # Order by comparison then param for readability
    if not W.empty:
        W = W.sort_values(["comparison", "prompt_category", "param"])  # stable order
        for _, r in W.iterrows():
            # Pretty experiment and prompt labels
            try:
                sideA_raw = str(r.get('sideA', ''))
                sideB_raw = str(r.get('sideB', ''))
                pc_raw = str(r.get('prompt_category', ''))
            except Exception:
                sideA_raw = str(r['sideA'])
                sideB_raw = str(r['sideB'])
                pc_raw = str(r['prompt_category'])
            nameA = exp_name_map.get(sideA_raw, sideA_raw)
            nameB = exp_name_map.get(sideB_raw, sideB_raw)
            pc_label = PC_PRINT_MAP.get(pc_raw.lower(), pc_raw)
            cond = f"{nameA} $\\to$ {nameB}"
            cond = _latex_escape(cond)
            # Ensure math arrow prints correctly: replace escaped backslashes with \to
            # cond = cond.replace("\\textbackslash{}\\textbackslash{}to", "\\to")
            # cond = cond.replace("\\textbackslash{}to", "\\to")
            cond = cond.replace("\\textbackslash{}\\textbackslash{}to", "-")
            cond = cond.replace("\\textbackslash{}to", "-")
            # Keep math in parameter label (e.g., $m_1$, $m_2$) unescaped
            param = _pretty_param(str(r["param"]))
            pc_out = _latex_escape(str(pc_label))
            most_s = _latex_escape(str(r.get(f"top_{k}_most", "")))
            least_s = _latex_escape(str(r.get(f"top_{k}_least", "")))
            lines.append(f"{cond} & {pc_out} & {param} & {most_s} & {least_s} \\\ \n")

    lines.append("\\hline\n\\end{tabular}\n\\end{table}\n")

    tex_path = out_dir / f"top_{k}_agents_experiment_pairs.tex"
    tex_path.write_text("".join(lines))

    return csv_path, tex_path




# ---- Top-k agents export from pcnum→pccot master ----
def export_topk_agents_from_pcnum_to_pccot_master(master_csv, k: int = 3):
    """Extract top-k most/least changing agents per experiment (pcnum→pccot) and
    export both a long-form CSV and a compact LaTeX table.

    Inputs
    - master_csv: Path to results/parameter_analysis/cbn_agg/topk_master_pcnum_to_pccot.csv
    - k: number of agents to include in most/least lists (default 3)

    Groups by (experiment_prompt_category, comparison, sideA, sideB, experiment, param)
    where sideA should be 'pcnum' and sideB 'pccot'.
    """
    mpath = Path(master_csv)
    if not mpath.exists():
        raise FileNotFoundError(f"Master CSV not found: {mpath}")

    out_dir = mpath.parent / "top_k"
    out_dir.mkdir(parents=True, exist_ok=True)

    df = pd.read_csv(mpath)
    # Ensure numeric fields
    for c in ["rank", "abs_delta", "delta", "pcnum", "pccot"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")

    # Fallback construct abs_delta if missing
    if "abs_delta" not in df.columns and "delta" in df.columns:
        df["abs_delta"] = df["delta"].abs()

    # Expected identifiers
    for c in [
        "experiment_prompt_category", "comparison", "sideA", "sideB",
        "experiment", "param", "rank_type", "agent"
    ]:
        if c not in df.columns:
            df[c] = None

    def _topk(sub: pd.DataFrame, kind: str) -> pd.DataFrame:
        X = sub[sub["rank_type"] == kind].copy()
        if X.empty:
            return X
        if "rank" in X.columns and X["rank"].notna().any():
            X = X.sort_values(["rank", "abs_delta"], ascending=[True, False])
        else:
            X = X.sort_values(["abs_delta", "agent"], ascending=[False, True]) if kind == "most" \
                else X.sort_values(["abs_delta", "agent"], ascending=[True, True])
        return X.head(k)

    group_cols = [
        "experiment_prompt_category", "comparison", "sideA", "sideB", "experiment", "param"
    ]

    # Long-form rows
    long_rows: list[dict] = []
    for keys, sub in df.groupby(group_cols, dropna=False):
        most = _topk(sub, "most")
        least = _topk(sub, "least")
        for tag, X in [("most", most), ("least", least)]:
            for _, r in X.iterrows():
                long_rows.append({
                    "experiment_prompt_category": r.get("experiment_prompt_category"),
                    "comparison": r.get("comparison"),
                    "sideA": r.get("sideA"),
                    "sideB": r.get("sideB"),
                    "experiment": r.get("experiment"),
                    "param": r.get("param"),
                    "rank_type": tag,
                    "rank": int(r.get("rank")) if pd.notna(r.get("rank")) else np.nan,
                    "agent": r.get("agent"),
                    "delta": float(r.get("delta")) if pd.notna(r.get("delta")) else np.nan,
                    "abs_delta": float(r.get("abs_delta")) if pd.notna(r.get("abs_delta")) else np.nan,
                })

    L = pd.DataFrame(long_rows)
    csv_path = out_dir / f"top_{k}_agents_pcnum_to_pccot.csv"
    L.to_csv(csv_path, index=False)

    # LaTeX helpers
    def _latex_escape(s: str) -> str:
        if s is None:
            return ""
        s = str(s)
        return s.replace("\\", "\\textbackslash{}") \
                .replace("_", "\\_") \
                .replace("%", "\\%") \
                .replace("&", "\\&")

    def _pretty_param(p: str) -> str:
        return {"b": "$b$", "m1": "$m_1$", "m2": "$m_2$", "pCavg": "$p(C)$"}.get(str(p), str(p))

    # Wide aggregation: one row per experiment×param
    wide_rows: list[dict] = []
    for keys, sub in L.groupby(group_cols, dropna=False):
        most = sub[sub["rank_type"] == "most"].sort_values("rank")
        least = sub[sub["rank_type"] == "least"].sort_values("rank")

        def mk(s: pd.DataFrame) -> str:
            out: list[str] = []
            for row in s.itertuples():
                try:
                    d = float(row.delta)
                except Exception:
                    d = np.nan
                out.append(f"{row.agent} ({d:+.3f})" if pd.notna(d) else f"{row.agent} (nan)")
            return "; ".join(out)

        wide_rows.append({
            "experiment": keys[4],
            "pc_pair": f"{PC_PRINT_MAP.get(str(keys[2]).lower(), keys[2])} $\\to$ {PC_PRINT_MAP.get(str(keys[3]).lower(), keys[3])}",
            "param": keys[5],
            f"top_{k}_most": mk(most),
            f"top_{k}_least": mk(least),
        })

    W = pd.DataFrame(wide_rows)

    # Compose LaTeX: Experiment, PC Pair, Param, Most, Least
    lines: list[str] = []
    lines.append("% Auto-generated; do not edit by hand\n")
    lines.append("\\begin{table}[t]\n\\centering\n")
    lines.append("\\scriptsize\n")
    lines.append(f"\\caption{{Top-{k} most/least-changing agents per prompt-category (PC) comparisons  (by signed $\\Delta$; $\\Delta$ = A$-$B) for given experiment and CBN parameter (Param).}}\n")
    lines.append("\\label{tab:top_%d_agents_pcnum_to_pccot}\n" % k)
    lines.append("\\begin{tabular}{l l l p{5cm} p{5cm}}\n\\hline\n")
    lines.append("Experiment & PC Pair & Param & Most & Least \\\\ \n\\hline\n")

    if not W.empty:
        W = W.sort_values(["experiment", "param"])  # stable order
        for _, r in W.iterrows():
            exp_name = exp_name_map.get(str(r.get("experiment")), str(r.get("experiment")))
            exp_out = _latex_escape(exp_name)
            pair = str(r.get("pc_pair"))
            # clean any escaped arrow in pair if needed
            pair = pair.replace("\\textbackslash{}\\textbackslash{}to",  "-").replace("\\textbackslash{}to",  "-")
                
            param = _pretty_param(str(r.get("param")))
            most_s = _latex_escape(str(r.get(f"top_{k}_most", "")))
            least_s = _latex_escape(str(r.get(f"top_{k}_least", "")))
            lines.append(f"{exp_out} & {pair} & {param} & {most_s} & {least_s} \\\\ \n")

    lines.append("\\hline\n\\end{tabular}\n\\end{table}\n")

    tex_path = out_dir / f"top_{k}_agents_pcnum_to_pccot.tex"
    tex_path.write_text("".join(lines))

    return csv_path, tex_path







def tex_model_coefs(tab: pd.DataFrame, caption: str, label: str, out_path: Path) -> None:
    """Write a LaTeX table for model coefficients (R^2 mixed model or GLM).

    Robust to varying column names and NaNs; displays Term, Coef, SE, and p.
    Complements mixed_effects_R2.csv or glm_three_param_share.csv, respectively.
    """
    if tab is None or tab.empty:
        out_path.write_text("% (no model coefficients to report)\n")
        return

    keep = tab.copy()

    # Normalize column names if they vary by model type
    rename_map = {
        "Coef.": "coef", "coef": "coef",
        "Std.Err.": "std_err", "std_err": "std_err",
        "P>|t|": "p_value", "P>|z|": "p_value", "p_value": "p_value",
        "index": "term"
    }
    for k, v in rename_map.items():
        if k in keep.columns and v not in keep.columns:
            keep = keep.rename(columns={k: v})

    # Ensure required columns exist
    for col in ["term", "coef", "std_err", "p_value"]:
        if col not in keep.columns:
            keep[col] = np.nan

    # Coerce numerics safely
    keep["coef"] = pd.to_numeric(keep["coef"], errors="coerce")
    keep["std_err"] = pd.to_numeric(keep["std_err"], errors="coerce")
    keep["p_value"] = pd.to_numeric(keep["p_value"], errors="coerce")

    # Only keep display columns
    keep = keep[["term", "coef", "std_err", "p_value"]].copy()

    def fmtf(x, dec=3, empty="--"):
        try:
            x = float(x)
            if not np.isfinite(x):
                return empty
            return f"{x:.{dec}f}"
        except Exception:
            return empty

    def fmtp(x, empty="--"):
        try:
            x = float(x)
            if not np.isfinite(x):
                return empty
            # compact scientific if very small
            return f"{x:.3g}"
        except Exception:
            return empty

    lines = []
    lines.append("% Auto-generated; do not edit by hand\n")
    lines.append("\\begin{table}[t]\n\\centering\n")
    lines.append(f"\\caption{{{caption}}}\n")
    lines.append(f"\\label{{{label}}}\n")
    lines.append("\\begin{tabular}{l r r r}\n\\toprule\n")
    lines.append("Term & Coef & SE & $p$ \\\\\n\\midrule\n")
    for _, r in keep.iterrows():
        term = latex_escape(r["term"])
        lines.append(f"{term} & {fmtf(r['coef'])} & {fmtf(r['std_err'])} & {fmtp(r['p_value'])} \\\\\n")
    lines.append("\\bottomrule\n\\end{tabular}\n\\end{table}\n")
    out_path.write_text("".join(lines))

# ------------------------- Main -------------------------

def parse_args() -> argparse.Namespace:
    """Parse CLI arguments for root paths, experiment selection, and outputs."""
    ap = argparse.ArgumentParser(description="Aggregate cross-experiment CBN analyses; export CSVs, PDFs, LaTeX.")
    ap.add_argument("--root", default=str(PROJECT_ROOT / "results" / "parameter_analysis"),
                    help="Root containing <experiment>/<tag>/normat_analysis/")
    ap.add_argument("--experiments", nargs="*", help="Optional list of experiments to include; default: discover all.")
    ap.add_argument("--baseline", default="rw17_indep_causes",
                    help="Baseline experiment for experiment-pair comparisons (default: rw17_indep_causes).")
    ap.add_argument("--out-root", default=None,
                    help="Output root for aggregated artifacts. Default: <root>/cbn_agg")
    ap.add_argument("--export-tex", action="store_true", help="Write LaTeX tables for key summaries.")
    ap.add_argument("--plots", action="store_true", help="Write PDF plots for summaries and paired comparisons.")
    return ap.parse_args()


def main() -> int:
    """End-to-end aggregation pipeline.

    Steps
    -----
    1) Build and save master_table.csv under <root>/cbn_agg/.
    2) Save per_condition_medians.csv (and optional LaTeX table).
    3) For each experiment, compute pcnum vs pccot paired comparisons; save CSVs, plots, LaTeX.
    4) For a chosen baseline experiment, compare against others within each prompt-category; save artifacts.
    5) Fit mixed-effects models for R^2 and a GLM for 3-param winner; save CSVs (and optional LaTeX).
    6) Generate overview R^2 boxplots by experiment × prompt-category.

    Returns
    -------
    int
        Exit status (0 on success).
    """
    args = parse_args()
    root = Path(args.root)
    out_root = Path(args.out_root) if args.out_root else (root / "cbn_agg")
    out_root.mkdir(parents=True, exist_ok=True)

    # 1) Build master table
    master = build_master(root, experiments=args.experiments)
    if master.empty:
        print("[WARN] No data discovered. Check --root.")
        return 0

    master_path = out_root / "master_table.csv"
    master.to_csv(master_path, index=False)
    print(f"Saved master table: {master_path}")

    # 2) Per-condition medians & share of 3-param winners (CSV + optional LaTeX)
    per_cond_rows = []
    for (exp, pc), sub in master.groupby(["experiment", "prompt_category"], dropna=False):
        def med(s): return float(np.median(to_num(s).dropna())) if not to_num(s).dropna().empty else np.nan
        per_cond_rows.append({
            "experiment": exp, "prompt_category": pc,
            "N_agents": int(sub["agent"].nunique()),
            "avg-tokens": avg_tokens_for(exp, pc),
            "median_R2": med(sub["loocv_r2"]),
            "median_b": med(sub["b"]),
            "median_m1": med(sub["m1"]),
            "median_m2": med(sub["m2"]),
            "median_pC": med((to_num(sub["pC1"]) + to_num(sub["pC2"])) / 2.0),
            "share_3param": share_three_param(sub["params_tying"]),
        })
    per_cond = pd.DataFrame(per_cond_rows).sort_values(["experiment", "prompt_category"])
    per_cond_path = out_root / "per_condition_medians.csv"
    per_cond.to_csv(per_cond_path, index=False)
    print(f"Saved per-condition medians: {per_cond_path}")
    if args.export_tex:
        tex_per_condition_medians(master, out_root / "per_condition_medians.tex")
        print(f"Saved LaTeX: {out_root / 'per_condition_medians.tex'}")

    make_prompt_category_pairs_for_all_experiments(master, out_root=out_root, do_plots=args.plots, export_tex=args.export_tex)

    # 3) Experiment-pair comparisons vs baseline (within prompt_category)
    experiments = sorted(master["experiment"].unique().tolist())
    if args.baseline not in experiments:
        print(f"[WARN] baseline '{args.baseline}' not found among experiments: {experiments}")
    else:
        others = [e for e in experiments if e != args.baseline]
        pcs = ["pcnum", "pccot"]
        for pc in pcs:
            for exp in others:
                W = matched_agents(master, args.baseline, exp, pc)
                if W.empty:
                    print(f"[INFO] Skipping {args.baseline} vs {exp} for {pc}: no matched agents across experiments.")
                    continue
                # deltas and summary
                deltas, summary = paired_summary_table(
                    W,
                    lhs_suffix=f"__{args.baseline}",
                    rhs_suffix=f"__{exp}",
                    label_cols={"baseline": "expA", "other": "expB", "prompt_category": "prompt_category"}
                )
                pair_dir = out_root / "experiment_pairs" / f"{args.baseline}__vs__{exp}" / pc
                pair_dir.mkdir(parents=True, exist_ok=True)
                deltas.to_csv(pair_dir / "paired_deltas.csv", index=False)
                summary.to_csv(pair_dir / "paired_summary.csv", index=False)
                print(f"Saved experiment-pair CSVs: {pair_dir}")

                # Write per-pair top-k parameter differences with composite key for filtering
                try:
                    T_pair = collect_topk_rows_exp_pair(
                        W,
                        lhs_suffix=f"__{args.baseline}",
                        rhs_suffix=f"__{exp}",
                        expA=args.baseline,
                        expB=exp,
                        prompt_category=pc,
                        k=10,
                    )
                    if not T_pair.empty:
                        T_pair.to_csv(pair_dir / "topk_master_exp_pair.csv", index=False)
                except Exception:
                    pass


                # Plots
                if args.plots:
                    # title shown within the generated plot
                    # plot_slope_R2_pair(
                    #     W,
                    #     colA=f"loocv_r2__{args.baseline}",
                    #     colB=f"loocv_r2__{exp}",
                    #     title=title,
                    #     out_path=pair_dir / "slope_R2.pdf"
                    # )
                    # plot_slope_R2_pair(W, labelA=f"{args.baseline} ({pc})", labelB=f"{exp} ({pc})",
                    #                     out_path=pair_dir / "slope_R2.pdf", baseline_col="loocv_r2__" + args.baseline)
                    plot_slope_R2_pair(
                        W,
                        labelA=f"{args.baseline} ({pc})",
                        labelB=f"{exp} ({pc})",
                        out_path=pair_dir / "slope_R2.pdf",
                        colA=f"loocv_r2__{args.baseline}",
                        colB=f"loocv_r2__{exp}",
                    )

                    # Parameter comparison plots (A → B)
                    plot_slope_params_exp_pair(
                        W,
                        suffixA=f"__{args.baseline}",
                        suffixB=f"__{exp}",
                        labelA=f"{args.baseline} ({pc})",
                        labelB=f"{exp} ({pc})",
                        out_dir=pair_dir,
                    )
                    plot_param_delta_violin_exp_pair(
                        W,
                        suffixA=f"__{args.baseline}",
                        suffixB=f"__{exp}",
                        out_path=pair_dir / "delta_violin_params_A_to_B.pdf",
                    )

                    # Combined slopes for R^2 and parameters with shared legend
                    plot_combined_slopes_exp_pair(
                        W,
                        labelA=f"{exp_name_map.get(args.baseline, args.baseline)} ({PC_PRINT_MAP.get(pc, pc)})",
                        labelB=f"{exp_name_map.get(exp, exp)} ({PC_PRINT_MAP.get(pc, pc)})",
                        suffixA=f"__{args.baseline}",
                        suffixB=f"__{exp}",
                        out_path=pair_dir / "combined_slopes_R2_and_params.pdf",
                    )



                # LaTeX
                if args.export_tex:
                    tex_paired_summary(
                        summary,
                        caption=f"Paired deltas ({pc}) for {args.baseline}→{exp} (matched agents).",
                        label=f"tab:paired_{pc}_{args.baseline}_to_{exp}",
                        out_path=pair_dir / "paired_summary.tex"
                    )

    # 4) Prompt-category comparisons within each experiment
    for exp in experiments:
        W = matched_pc(master, exp)
        if W.empty:
            continue
        deltas, summary = paired_summary_table(
            W,
            lhs_suffix="__pcnum",
            rhs_suffix="__pccot",
            label_cols={"experiment": "experiment"}
        )
        pc_dir = out_root / "prompt_category_pairs" / exp
        pc_dir.mkdir(parents=True, exist_ok=True)
        deltas.to_csv(pc_dir / "paired_deltas_pcnum_to_pccot.csv", index=False)
        summary.to_csv(pc_dir / "paired_summary_pcnum_to_pccot.csv", index=False)
        print(f"Saved pcnum vs pccot CSVs: {pc_dir}")

        if args.plots:
            # title shown in slope plot itself; no separate variable needed
            # plot_slope_R2_pair(
            #     W,
            #     colA="loocv_r2__pcnum",
            #     colB="loocv_r2__pccot",
            #     title=title,
            #     out_path=pc_dir / "slope_R2_pcnum_to_pccot.pdf",
            #     labelA="Numeric",
            #     labelB="CoT"
            # )
            # plot_slope_R2_pair(W, labelA="Numeric", labelB="CoT",
            #        out_path=pc_dir / "slope_R2_pcnum_to_pccot.pdf", baseline_col="loocv_r2_pcnum")
            plot_slope_R2_pair(
                W,
                labelA="Numeric",
                labelB="CoT",
                out_path=pc_dir / "slope_R2_pcnum_to_pccot.pdf",
                colA="loocv_r2__pcnum",
                colB="loocv_r2__pccot",
            )
            # New parameter plots
            plot_slope_params_pcnum_to_pccot(W, pc_dir)
            plot_param_delta_violin_pcnum_to_pccot(W, pc_dir / "delta_violin_params_pcnum_to_pccot.pdf")
            # Top-k most/least changed agents per parameter
            report_topk_param_differences_pcnum_to_pccot(W, pc_dir, k=10)

            # Also emit a master top-k file with a composite key for easier filtering
            try:
                T_master = collect_topk_rows_pcnum_to_pccot(W, k=10)
                if not T_master.empty:
                    T_master.to_csv(pc_dir / "topk_master_pcnum_to_pccot.csv", index=False)
            except Exception as _e:
                # Non-fatal: continue the pipeline even if this optional artifact fails
                pass

            # Combined slopes (R^2 + params) with shared legend
            plot_combined_slopes_pcnum_to_pccot(W, pc_dir / "combined_slopes_R2_and_params_pcnum_to_pccot.pdf")



        if args.export_tex:
            tex_paired_summary(
                summary,
                caption=f"Paired deltas within {exp} (pcnum→pccot; matched agents).",
                label=f"tab:paired_pc_{exp}",
                out_path=pc_dir / "paired_summary_pcnum_to_pccot.tex"
            )

    # 5) Mixed-effects models on the full (unbalanced) set
    #    5a) LOOCV R^2
    coef_R2 = fit_mixed_R2(master, experiments)
    coef_R2_path = out_root / "mixed_effects_R2.csv"
    coef_R2.to_csv(coef_R2_path, index=False)
    print(f"Saved mixed-effects (R^2) coefficients: {coef_R2_path}")
    if args.export_tex:
        tex_model_coefs(
            coef_R2,
            caption="Mixed-effects coefficients for LOOCV $R^2$ (Gaussian MixedLM or OLS-FE fallback).",
            label="tab:mixed_R2",
            out_path=out_root / "mixed_effects_R2.tex"
        )
        print(f"Saved LaTeX: {out_root / 'mixed_effects_R2.tex'}")

    #    5b) AIC winner: 3-parameter vs 4-parameter
    coef_3par = fit_glm_three_param(master)
    coef_3par_path = out_root / "glm_three_param_share.csv"
    coef_3par.to_csv(coef_3par_path, index=False)
    print(f"Saved GLM (3-param share) coefficients: {coef_3par_path}")
    if args.export_tex:
        tex_model_coefs(
            coef_3par,
            caption="GLM (Binomial) coefficients for 3-parameter winner (agent FE; cluster-robust SE by agent).",
            label="tab:glm_three_param",
            out_path=out_root / "glm_three_param_share.tex"
        )
        print(f"Saved LaTeX: {out_root / 'glm_three_param_share.tex'}")

    # 6) High-level plots (R^2 distributions)
    if args.plots:
        plot_box_R2_by_exp_pc(master, out_root / "box_R2_by_experiment_and_pc_beautiful.pdf")
        print(f"Saved plot: {out_root / 'box_R2_by_experiment_and_pc_beautiful.pdf'}")

        # plot_box_R2_by_experiment_and_pc(master, out_root / "box_R2_by_experiment_and_pc_new.pdf")
        # OLD: plot_box_R2_by_experiment_and_pc(master, out_root / "box_R2_by_experiment_and_pc_new.pdf")
        # caused a path join bug in your script

        # NEW: pass the directory (or a full file path; both work). Directory is simpler:
        plot_box_R2_by_experiment_and_pc(master, out_root)

    # New: parameter boxplots (2x2 panel)
    plot_box_params_by_experiment_and_pc(master, out_root)

        #print(f"Saved plot: {out_root / 'box_R2_by_experiment_and_pc_new.pdf'}")

    # Aggregate per-experiment master top-k files (if any) into one master CSV at the root
    try:
        topk_paths = sorted((out_root / "prompt_category_pairs").glob("*/topk_master_pcnum_to_pccot.csv"))
        if topk_paths:
            frames = [pd.read_csv(p) for p in topk_paths]
            T_all = pd.concat(frames, ignore_index=True)
            # Ensure column order consistency if differing subsets occur
            desired_order = [
                "experiment_prompt_category", "comparison", "sideA", "sideB",
                "agent", "param", "pcnum", "pccot", "delta", "abs_delta",
                "rank_type", "rank", "experiment", "prompt_category"
            ]
            cols = [c for c in desired_order if c in T_all.columns] + [c for c in T_all.columns if c not in desired_order]
            T_all = T_all[cols]
            out_master = out_root / "topk_master_pcnum_to_pccot.csv"
            T_all.to_csv(out_master, index=False)
            print(f"Saved master top-k across experiments: {out_master}")
    except Exception:
        # Non-fatal; continue
        pass

    # Aggregate per-experiment master top-k files (if any) into one master CSV at the root
    try:
        topk_paths = sorted((out_root / "prompt_category_pairs").glob("*/topk_master_pcnum_to_pccot.csv"))
        if topk_paths:
            frames = [pd.read_csv(p) for p in topk_paths]
            T_all = pd.concat(frames, ignore_index=True)
            desired_order = [
                "experiment_prompt_category", "comparison", "sideA", "sideB",
                "agent", "param", "pcnum", "pccot", "delta", "abs_delta",
                "rank_type", "rank", "experiment", "prompt_category"
            ]
            cols = [c for c in desired_order if c in T_all.columns] + [c for c in T_all.columns if c not in desired_order]
            T_all = T_all[cols]
            out_master = out_root / "topk_master_pcnum_to_pccot.csv"
            T_all.to_csv(out_master, index=False)
            print(f"Saved master top-k across experiments: {out_master}")
    except Exception:
        pass

    # Aggregate per-pair top-k files into a global master across prompt-categories
    try:
        pair_topk = sorted((out_root / "experiment_pairs").glob("*__vs__*/*/topk_master_exp_pair.csv"))
        if pair_topk:
            frames = [pd.read_csv(p) for p in pair_topk]
            T_pairs = pd.concat(frames, ignore_index=True)
            desired_order = [
                "exp_pair_prompt_category", "comparison", "sideA", "sideB", "prompt_category",
                "agent", "param", "valA", "valB", "delta", "abs_delta",
                "rank_type", "rank", "baseline_exp", "other_exp"
            ]
            cols = [c for c in desired_order if c in T_pairs.columns] + [c for c in T_pairs.columns if c not in desired_order]
            T_pairs = T_pairs[cols]
            out_pairs_master = out_root / "topk_master_experiment_pairs.csv"
            out_pairs_master_pc_pairs = out_root / "topk_master_pcnum_to_pccot.csv"
            T_pairs.to_csv(out_pairs_master, index=False)
            print(f"Saved master top-k for experiment pairs: {out_pairs_master}")

            # Also emit a compact top-k summary (CSV + LaTeX) under top_k/
            try:
                export_topk_agents_from_exp_pair_master(out_pairs_master, k=3)
                export_topk_agents_from_pcnum_to_pccot_master(out_pairs_master_pc_pairs, k=3)
            except Exception as _e:
                # Non-fatal: keep pipeline going even if top-k export fails
                print(f"[WARN] top-k export failed: {_e}")
    except Exception:
        pass

    print("\nDone. Artifacts under:", out_root.resolve())
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
