Metadata-Version: 2.4
Name: causalign
Version: 0.1.0
Summary: Understanding Causal Reasoning in LLMs
Home-page: https://github.com/hmd101/causalign
Author: HMD
Author-email: HMD <hmd8142@nyu.edu>
License: MIT
Project-URL: github, https://github.com/hmd101/causalign
Keywords: large language models,causality,causal inference
Platform: any
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Requires-Python: >=3.8,<3.14
Description-Content-Type: text/markdown
Requires-Dist: numpy>=1.21.3
Requires-Dist: torch>=2.0.0
Provides-Extra: plotting
Requires-Dist: matplotlib>=3.4.3; extra == "plotting"
Dynamic: author
Dynamic: home-page
Dynamic: requires-python

![MyPackage Logo](assets/logo.png)


# CausalAlign -- Evaluating Causal Alignment Between Humans and LLMs

## **Overview**

**CausalAlign** is a Python package designed to evaluate the causal reasoning abilities of **Large Language Models (LLMs)** in comparison to **human inference**. It provides tools to systematically assess how LLMs reason about causal structures—particularly **collider graphs** — and whether their judgments align with normative Bayesian inference or human reasoning biases.

This package is based on the work presented in the paper:

> "Do Large Language Models Reason Causally Like Us? Even Better?"
> 
> 
> *Hanna M. Dettki, Brenden M. Lake, Charley M. Wu, Bob Rehder*
> 
> [[arXiv:2502.10215]](https://arxiv.org/pdf/2502.10215)
> 

## **Key Features**

- Implements the **collider causal inference** framework to compare LLMs' and human reasoning.
- Provides tools for **(Spearman) correlation analysis** to measure alignment between human and LLM judgments.
- Fits responses to **Causal Bayesian Networks (CBNs)** to evaluate normative reasoning.
- Supports API calls for multiple LLMs, including **GPT-4o, GPT-3.5, Claude-3-Opus, and Gemini-Pro**.
     - for the dataset constructed based on experiment 1 (model-only condition) from [Rehder&Waldmann, 2017](https://link.springer.com/article/10.3758/s13421-016-0662-3).
     - dataset: _Coming soon_


## **Installation**

To install the package, clone the repository and install dependencies:

```bash
git clone <https://github.com/hmd101/causalign.git>
cd causalign
pip install -e .

```

If you want to install directly from a public GitHub repository:

```bash
pip install git+https://github.com/hmd101/causalign.git

```

## **Usage**

```bash
_Coming soon_
```


### **API calls**
```bash
_Coming soon_
```

#### **Required Data Format:**

```bash
_Coming soon_
```

## **Data and Models**

CausalAlign provides support for evaluating LLMs across different causal inference tasks. It is particularly apt for datasets building on experiments in  [Rehder&Waldmann, 2017](https://link.springer.com/article/10.3758/s13421-016-0662-3).



It includes built-in support for:

- **Human data from Rehder & Waldmann (2017), experiment 1, model-only condition**
- **LLM inference responses (GPT-4o, GPT-3.5, Claude-3-Opus, and Gemini-Pro)**

## **Data Processing: When to Aggregate Human Responses**

A critical decision in comparing human and LLM causal reasoning is whether to **aggregate human responses** per experimental condition. This choice has significant implications for downstream statistical analyses and interpretation.

### **The Sample Size Imbalance Problem**

In typical experiments, you have:
- **4+ human responses per experimental condition (task/domain combination)**
- **1 LLM response per experimental condition**

For example, in a collider experiment with 11 tasks × 3 domains = 33 conditions:
- **Human data**: 4 responses × 33 conditions = **132 total responses per participant group**
- **LLM data**: 1 response × 33 conditions = **33 total responses per model**

This creates an **imbalanced sample size** problem with several statistical implications:

### **Statistical Implications of Imbalanced Sample Sizes**

#### **1. Unequal Statistical Power**
- **Human responses**: Higher power to detect effects due to larger sample size
- **LLM responses**: Lower power with smaller effective sample size
- **Problem**: Makes it harder to fairly compare effect sizes and significance levels

#### **2. Biased Variance Estimation**
- **Humans**: Standard errors will be smaller (more precise estimates)
- **LLMs**: Standard errors will be larger (less precise estimates)  
- **Problem**: Confidence intervals and error bars will have different widths, making visual comparisons misleading

#### **3. Mixed-Effects Model Issues**
- **Unbalanced designs** can lead to biased random effect estimates
- **Convergence problems** in hierarchical models
- **Difficulty interpreting** subject-level vs. condition-level effects

#### **4. Correlation Analysis Bias**
- **Within-condition variability** for humans vs. single point estimates for LLMs
- **Inflated correlations** when comparing humans (with noise) to LLMs (point estimates)
- **Difficulty separating** systematic vs. random differences

### **When to Aggregate Human Responses**

#### **✅ Aggregate When:**

**1. Model Comparison Studies**
```python
# Comparing average human performance vs. each LLM
aggregate_human_responses=True
# Result: 33 human responses vs. 33 LLM responses per model
```

**2. Correlation Analysis**
```python
# Computing Spearman correlations between humans and LLMs
aggregate_human_responses=True
# Clean paired comparisons without sample size confounds
```

**3. Effect Size Calculations**
```python
# Cohen's d, standardized mean differences
aggregate_human_responses=True
# Fair comparison of central tendencies
```

**4. Visualization (Box Plots, Line Plots)**
```python
# When you want to show "typical" human vs. LLM performance
aggregate_human_responses=True
# Balanced visual comparison
```

#### **❌ Don't Aggregate When:**

**1. Individual Differences Research**
```python
# Studying human response variability and consistency
aggregate_human_responses=False
# Preserves individual-level variation
```

**2. Reliability Analysis**  
```python
# Computing intra-class correlations, split-half reliability
aggregate_human_responses=False
# Need multiple responses per condition
```

**3. Mixed-Effects Models with Proper Nesting**
```python
# Hierarchical models accounting for repeated measures
aggregate_human_responses=False
# Model handles unbalanced design appropriately
```

**4. Variance Decomposition**
```python
# Separating within-condition vs. between-condition variance
aggregate_human_responses=False
# Need individual responses to estimate variance components
```

### **Practical Example: Correlation Analysis**

**Without Aggregation (Problematic):**
```python
# Humans: 132 responses, LLMs: 33 responses
correlation = spearmanr(human_responses, llm_responses)
# Biased: comparing noisy human distribution to LLM point estimates
```

**With Aggregation (Recommended):**
```python
# Both humans and LLMs: 33 responses (one per condition)
correlation = spearmanr(human_avg_responses, llm_responses)  
# Clean: comparing human consensus to LLM responses
```

### **Pipeline Configuration**

```python
# For fair model comparison (recommended)
config = PipelineConfig(
    aggregate_human_responses=True,  # Balance sample sizes
    add_reasoning_types=True        # Add cognitive categories
)

# For individual differences research
config = PipelineConfig(
    aggregate_human_responses=False, # Preserve individual responses
    add_reasoning_types=True        # Keep cognitive categories
)
```

### **Recommendation**

For **most causal reasoning comparisons between humans and LLMs**, we recommend:
- **`aggregate_human_responses=True`** for balanced statistical comparisons
- Use **mixed-effects models** when you need to preserve individual-level data
- Always **report both approaches** if results differ substantially

This ensures fair statistical comparisons while acknowledging the inherent differences in how human and LLM data are collected.

## **Some Benchmarking Results:**


### **Paper Summary: "Do Large Language Models Reason Causally Like Us? Even Better?"**

Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, but do they **reason causally like humans**? This study investigates whether LLMs exhibit **human-like causal reasoning**, whether they follow **normative Bayesian inference**, or whether they introduce **unique biases**.

The study compares human judgments with four LLMs (**GPT-4o, GPT-3.5, Claude-3-Opus, and Gemini-Pro**) on **causal inference tasks** based on **collider structures** (e.g., `C1 → E ← C2`) embedded in three different knowledge domains (economy, sociology, meteorology) based on the cover stories introduced in Rehder & Waldmann /2017:



![alt text](image-1.png)


### **Causal Reasoning Varies Across LLMs and Humans**

We found that:

- **GPT-4o and Claude-3** demonstrated the most **normative reasoning**, adhering more closely to probability theory than human participants.
- **Gemini-Pro and GPT-3.5** leaned towards **associative reasoning**, relying on statistical correlations rather than strict causal logic.
- All models **deviated from expected causal independence**, but **Claude-3 exhibited the least bias**, making its responses the most mathematically consistent.

Interestingly, humans rely on **heuristics** that sometimes contradict pure probability theory. For example, the **"explaining away" effect**—where observing one cause makes another less likely—was recognized by AI models but **handled inconsistently**, depending on their training data.

### **AI and Humans Approach Causality Differently**

A key takeaway is that LLMs do not simply **replicate** human causal reasoning -- it **follows a different logic**, possibly due to applying domain knowledge-

- While human judgments remained relatively **stable** across different domains, AI models **adapted their reasoning** based on the specific context (e.g., economics vs. sociology).
- **GPT-4o treated causal relationships as most deterministic**, assuming that certain causes always lead to specific effects, especially in the economy domain for diagnostic reasoning types (E=0).
- Humans, on the other hand, **embrace uncertainty**, recognizing that possibly other caues, not stated in the prompt may have influence. An alternative explanation could be "lazy reasoning", defaulting to what the experimental setup had set as default (here, 50, where humans were supposed to move a slider on a range from 0-100 when providing their likelihood judgments).


![alt text](image.png)

We find that LLMs generally can do the causal inference task and follow relatively similar reasoning patterns as humans do. However, there are domain- and task-speicific differences. 
This work underscores the need to evaluate LLMs’ reasoning **beyond pattern recognition**, ensuring they make **reliable causal inferences** in **real-world applications**.

For more details, refer to the full paper: [[arXiv:2502.10215]](https://arxiv.org/pdf/2502.10215).

Table: Spearman correlations (**rₛ**) between **human** and **LLM** inferences.

| Model | Economy (rₛ) | Sociology (rₛ) | Weather (rₛ) | Pooled across all domains (rₛ) |
| --- | --- | --- | --- | --- |
| **Claude** | 0.557 | **0.637 | **0.698** | **0.631** |
| **GPT-4o** | **0.662** | 0.572 | 0.645 | **0.626** |
| **GPT-3.5** | 0.419 | 0.450 | **0.518** | 0.462 |
| **Gemini** | 0.393 | 0.297 | **0.427** | 0.372 |


# Package organization

```
src/causalign/
├── prompts/                      # (formerly causal_task_generation)
│   ├── core/                    # Core prompt generation logic
│   │   ├── domain.py           # Domain-specific logic
│   │   ├── verbalization.py    # Story verbalization
│   │   ├── prompt.py          # Prompt templates
│   │   └── constants.py       # Shared constants
│   ├── generators/             # Different graph type generators
│   │   ├── collider.py
│   │   ├── fork.py
│   │   └── chain.py
│   ├── versions/              # Different prompt versions
│   │   ├── rw17/             # Rehder & Waldmann 2017
│   │   ├── abstract/
│   │   └── fantasy/
│   └── notebooks/            # Prompt generation notebooks
│
├── data/                      # Data management
│   ├── raw/                  # Raw data
│   │   ├── human/           # Human response data
│   │   └── llm/            # Raw LLM responses
│   ├── processed/           # Processed data
│   │   ├── human/          # Processed human data
│   │   └── llm/           # Processed LLM responses
│   └── merged/             # Merged datasets
│       └── rw17/          # RW17 specific merged data
│
├── experiment/               # Experiment execution
│   ├── api/                # API clients and configuration
│   │   ├── base.py
│   │   ├── openai.py
│   │   └── anthropic.py
│   └── config/            # Experiment configuration
│
└── analysis/               # Analysis and visualization
    ├── processing/        # Data processing
    │   ├── response_parser.py    # XML parsing
    │   ├── roman_numerals.py    # Roman numeral conversion
    │   └── data_merger.py       # Merging human and LLM data
    ├── visualization/     # Plotting and visualization
    │   ├── line_plots.py
    │   └── detailed_responses.py
    └── notebooks/        # Analysis notebooks
```

## **Citation**


Please cite paper as:

```bibtex
@misc{dettki2025largelanguagemodelsreason,
  title         = {Do Large Language Models Reason Causally Like Us? Even Better?},
  author        = {Hanna M. Dettki and Brenden M. Lake and Charley M. Wu and Bob Rehder},
  year          = {2025},
  eprint        = {2502.10215},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2502.10215}
}
```
